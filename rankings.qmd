---
title: "Open Access in institutionellen Rankings"
format: 
  html: 
    resources: 
      - shinylive-sw.js
filters:
  - shinylive
bibliography: rankings/data/rankings.bib
csl: apa.csl
lightbox:
  match: auto
crossref:
  fig-title: "Abbildung"
  fig-prefix: "Abb."
  tbl-title: "Tabelle"
  tbl-prefix: "Tab."
---

Institutionelle oder Hochschulrankings genießen eine hohe Aufmerksamkeit in Wissenschaftspolitik und Öffentlichkeit, weil es ihnen gelingt, wissenschaftliche Einrichtungen auf globaler Ebene übersichtlich miteinander zu vergleichen. Sie dienen verschiedenen Interessengruppen, wie politischen Entscheidungsträgern, Hochschulmanagern oder Studieninteressierten, unter anderem als Instrument zur strategischen Entscheidungsfindung, Veranschaulichung der institutionellen Exzellenz oder bei der Wahl des Studienorts [@Waltman2012]. Die jeweilige Methodik, auf der die Rankings basieren, ist mitunter vielzähliger Kritikpunkte ausgesetzt, beispielsweise wegen ihrer Tendenz, mehrere Dimensionen des Leistungsspektrums wissenschaftlicher Einrichtungen zu einer einzigen, oft undurchsichtigen Punktzahl zusammenzufassen [@vanRaan2005;@Piro2016;@Diprose2023]. Dieser Ansatz erschwert nicht nur die Interpretation der Rankings, sondern berücksichtigt auch nicht hinreichend die unterschiedlichen Aufgaben und Prioritäten der Einrichtungen [@Waltman2012;@Huang2020].

In den letzten Jahren hat die Einbeziehung von Open-Access-Kennzahlen in Hochschulrankings an Bedeutung gewonnen. Open Access bezeichnet den freien und uneingeschränkten Online-Zugang zu Forschungsergebnissen [@Suber2012] und ist zu einem wichtigen Indikator für das Engagement einer wissenschaftlichen Einrichtung für eine offene und transparante Wissenschaftspraxis geworden. Diese Entwicklung wird zusätzlich durch die Vorgaben von Förderorganisationen unterstützt, die Open Access zunehmend als Voraussetzung für die Forschungsförderung betrachten [@Wellcome_Trust;@NIH;@DFG;@Fournier2017;@Huang2020a;@Ruecknagel2021]. Diese Vorgaben setzen neben dem Selbstverständnis vieler Einrichtungen einen zusätzlichen Anreiz Forschungsergebnisse im Open Access zugänglich zu machen. Die Integration von Open-Access-Kennzahlen in Hochschulrankings ermöglicht den Aktivitäten der Einrichtungen im Bereich Open Access Rechnung zu tragen und bietet eine zusätzliche Vergleichsdimension [@Diprose2023].

Es konnten fünf institutionelle Rankings mit Open Access als Vergleichsdimension identifiziert werden: das Leiden Ranking 2024 (LR), das Leiden Ranking Open Edition 2024 (LROE), das SCImago Institutions Ranking 2024 (SIR), das Quacquarelli Symonds World University Rankings: Sustainability 2025 (QS) und das COKI OA Dashboard 2025 (COKI). Alle fünf Rankings enthalten einen oder mehrere Open-Access-Indikatoren und sind hinsichtlich ihrer geografischen Abdeckung global ausgerichtet. 

Diese Seite bietet eine Übersicht über die Charakteristika der fünf Rankings, sowie eine Untersuchung dazu, inwieweit die Ergebnisse der Rankings miteinander vergleichbar sind. Dies beinhaltet unter anderem einen Vergleich der von den Rankings verwendeten Datenquellen und Open-Access-Indikatoren, sowie eine Erhebung dazu, in welchem Umfang sich die Einrichtungen an der Datenerhebung durch die Rankinganbieter beteiligen können. Der Fokus liegt dabei insbesondere auf der Open-Access-Dimension.


## Institutionellen Rankings mit Open Access als Vergleichsdimension

Die folgenden Abschnitte beginnen mit einer Beschreibung der Kernmerkmale der ausgewählten Rankings. Am Ende jedes Abschnitts werden Details zu den Rankings in einem aufklappbaren Textkasten mit der Überschrift „Ranking-Details“ zusammengefasst. Dazu gehört insbesondere eine umfassende Liste der in jedem Ranking verwendeten Open-Access-Indikatoren. 


### Leiden Ranking

Das Leiden Ranking wird seit 2007 jährlich vom Centre for Science and Technology Studies (CWTS) der Universität Leiden in den Niederlanden erstellt [@Waltman2012;@LR_updates]. Das Ranking bietet keine klassische Rangliste, bei der eine Gesamtkennzahl berechnet wird, jedoch verschiedene bibliometrische Indikatoren entlang der Dimensionen *scientific impact*, *collaboration*, *Open Access* und *gender*. Die Indikatoren werden sowohl in größenabhängiger (absolute Zahlen) als auch in größenunabhängiger (relative Anteile) Form dargestellt, um Unterschiede in der Größe der Universitäten zu berücksichtigen. Das Ranking bietet auch Trendanalysen über Zeiträume und Stabilitätsintervalle, um statistische Unsicherheiten zu berücksichtigen [@LR_general;@LR_method]. 

Die Version 2024 des Leiden Ranking umfasst bibliometrische Daten für 1.506 Universitäten weltweit [@LR_method]. Indikatoren werden jeweils für Zeiträume von vier Jahren angegeben, die Version 2024 erweitert diese um den Zeitraum 2019--2022 [@LR_method]. Die bibliographischen Daten stammen aus dem Web of Science, wobei jedoch nur Artikel und Reviews berücksichtigt werden, die in internationalen wissenschaftlichen Zeitschriften veröffentlicht wurden [@LR_method]. Die Auswahl der Universitäten erfolgt auf der Grundlage ihrer Forschungsleistung, wobei diese mindestens 800 sogenannte *core publications* vorweisen müssen, um in das Ranking aufgenommen zu werden. Unter *core publications* werden Veröffentlichungen verstanden, die in internationalen wissenschaftlichen Zeitschriften in solchen Feldern veröffentlicht wurden, die für die Zitationsanalyse geeignet sind. Diese Publikationen müssen in englischer Sprache verfasst sein, mindestens eine:n Autor:in haben, dürfen nicht zurückgezogen (*retracted*) worden sein und müssen in einem sogenannten *core journal* erschienen sein. *core journals* zeichnen sich durch ihre internationale Ausrichtung aus und müssen in einem Feld angesiedelt sein, das für die Zitationsanalyse geeignet ist. Das Leiden Ranking verwendet keine Daten, die direkt von Universitäten stammen [@LR_method;@LR_general].

Ein wesentliches Merkmal des Leiden Ranking ist der Fokus auf Open-Access-Veröffentlichungen. Mit der Version 2019 des Rankings wurden Indikatoren zum Open-Access-Publizieren hinzugefügt [@LR_updates]. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das Leiden Ranking OpenAlex als Datenquelle [@LR_method;@LR_general].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2024

**Website:** <https://www.leidenranking.com/>

**Ersteller:** [Centre for Science and Technology Studies (CWTS)](https://www.universiteitleiden.nl/en/social-behavioural-sciences/cwts)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://traditional.leidenranking.com/Content/CWTS%20Leiden%20Ranking%202024.pdf>

**Datenquellen:** Web of Science (Publikationen), OpenAlex (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Open Access

**Open Access Indikatoren (Gewichtung):**

- Gesamtzahl der Veröffentlichungen einer Universität
- Anzahl und Anteil der Open-Access-Veröffentlichungen einer Universität
- Anzahl und Anteil der Gold-Open-Access-Veröffentlichungen einer Universität
- Anzahl und Anteil der Hybrid-Open-Access-Publikationen einer Universität
- Die Anzahl und der Anteil der Bronze-Open-Access-Publikationen einer Universität
- Die Anzahl und der Anteil der grünen Open-Access-Publikationen einer Universität
- Die Anzahl und der Anteil der Publikationen einer Universität, deren Open-Access-Status unbekannt ist

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.506 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Ranking

**Datenverfügbarkeit:** Ergebnisdaten verfügbar unter CC-BY Lizenz: @LR2024_data

:::


### Leiden Ranking Open Edition


Das Leiden Ranking Open Edition wird seit 2024 [@LROE2024] jährlich vom Centre for Science and Technology Studies (CWTS) der Universität Leiden in den Niederlanden erstellt. Es ist eine annähernde Reproduktion des traditionellen Leiden Ranking unter Nutzung offener Datenquellen [@LROE2024] und bietet ebenfalls keine klassische Rangliste, sondern verschiedene bibliometrische Indikatoren entlang der Dimensionen *scientific impact*, *collaboration*, und *Open Access*. Die Indikatoren werden in der Open Edition ebenfalls sowohl in größenabhängiger (absolute Zahlen) als auch in größenunabhängiger (relative Anteile) Form dargestellt, um Unterschiede in der Größe der Universitäten zu berücksichtigen. Die Open Edition des Leiden Ranking bietet zudem ebenfalls Trendanalysen über Zeiträume und Stabilitätsintervalle, um statistische Unsicherheiten zu berücksichtigen [@LROE_method;@LROE_updates]. 

Die Version 2024 des Leiden Ranking umfasst bibliometrische Daten für 1.506 Universitäten weltweit [@LROE_method]. Indikatoren werden jeweils für Zeiträume von vier Jahren angegeben, die Version 2024 erweitert diese um den Zeitraum 2019--2022 [@LROE_method]. Die bibliographischen Daten stammen aus OpenAlex, wobei jedoch nur sogenannte *core publications* berücksichtigt werden [@LROE_method]. Im Vergleich zum traditionellen Leiden Ranking zählen in der Open Edition zusätzlich Publikationen vom Typ Buchkapitel, die in Buchreihen veröffentlicht wurden zu den *core publications*. Ferner müssen alle Publikationen nicht nur Autor:innenangaben, sondern auch Affiliationsangaben und Referenzen enthalten, um zu den *core publications* zu zählen. Alle anderen Kriterien für die *core publications* und *core journals* sind deckungsgleich zum traditionellen Leiden Ranking [@LROE_method].

Auch das Leiden Ranking Open Edition legt einen Fokus auf Open-Access-Veröffentlichungen und bietet weitestgehend die selben Open-Access-Indikatoren wie das traditionelle Leiden Ranking, mit Ausnahme der Indikatoren zu Publikationen mit unklarem Open-Access-Status [@LROE_method]. Darüber hinaus weist das Leiden Ranking Open Edition die Besonderheit auf, dass auf der Weboberfläche die den Indikatoren zugrunde liegenden Publikationen einsehen werden können [@LROE2024a].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2024

**Website:** <https://www.open.leidenranking.com/>

**Ersteller:** [Centre for Science and Technology Studies (CWTS)](https://www.universiteitleiden.nl/en/social-behavioural-sciences/cwts)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://open.leidenranking.com/Content/CWTS%20Leiden%20Ranking%20Open%20Edition%202024.pdf>

**Datenquellen:** OpenAlex

**Dimension mit Open Access Indikatoren (Gewichtung):** Open Access

**Open Access Indikatoren (Gewichtung):**

- Gesamtzahl der Veröffentlichungen einer Universität
- Anzahl und Anteil der Open-Access-Veröffentlichungen einer Universität
- Anzahl und Anteil der Gold-Open-Access-Veröffentlichungen einer Universität
- Anzahl und Anteil der Hybrid-Open-Access-Publikationen einer Universität
- Die Anzahl und der Anteil der Bronze-Open-Access-Publikationen einer Universität
- Die Anzahl und der Anteil der grünen Open-Access-Publikationen einer Universität

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.506 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Ranking

**Datenverfügbarkeit:** Rohdaten und Ergebnisdaten verfügbar unter CC0 Lizenz: 

- Rohdaten: @LROE2024_raw
- Ergebnisdaten: @LROE2024_results

:::

### SCImago Institutions Ranking

Das SCImago Institutions Ranking wird seit 2009 jährlich von der SCImago Research Group erstellt [@Rousseau2018]. Das Ranking bietet eine klassische Rangliste (league table) von wissenschaftlichen und forschungsnahen Einrichtungen anhand eines zusammengesetzten Indikators auf einer Skala zwischen 0 und 100. Der zusammengesetzte Indikator kombiniert Bewertungen entlang dreier Dimensionen: Forschung, Innovation und gesellschaftliche Wirkung. Der zusammengesetzte Indikator ergibt sich aus einer gewichtete Kombination von Metriken entlang dieser Dimensionen, wobei sowohl größenabhängige als auch größenunabhängige Metriken verwendet werden. Für jede Einrichtung bietet das Ranking zusätzlich noch eine Reihe von Grafiken beispielsweise zu der Rangentwicklung über die Zeit, Vergleichen auf nationaler, regionaler oder globaler Ebene, oder ähnlichen Institutionen gemäß des Publikationsprofils [@SIR_method].

Die Version 2025 des SCImago Institutions Ranking umfasst Daten für 9.756 Institutionen weltweit [@SIR]. Der zusammengesetzte Indikator wird pro Jahr für einen Zeitraum von fünf Jahren bis zwei Jahre vor Veröffentlichung des Rankings angegeben, für die Version 2025 also für den Zeitraum 2019--2023. Die bibliographischen Daten stammen aus Scopus. Die Auswahl der Institutionen erfolgt auf der Grundlage ihrer Veröffentlichungsleistung. Im letzten Jahr des betrachteten Zeitraums müssen die Institutionen mindestens 100 in Scopus indexierte Publikationen veröffentlicht haben, um in das Ranking aufgenommen zu werden. Ferner müssen mindestens 75% des gesamten Publikationsvolumens einer Einrichtung sogenannte zitierfähige Dokumente ausmachen. Zu den zitierfähigen Dokumenten werden für das SCImago Institutions Ranking die Publikationstypen Artikel, Buchkapitel, Konferenzbeiträge, Reviews und Short Surveys gezählt [@SIR_method].

Innerhalb der Forschungsdimension, die mit 50% in den zusammengesetzten Indikator einfließt, wurde mit der Version 2019 ein Open-Access-Indikator dem Ranking hinzugefügt. Dieser geht mit 2% Gewichtung in die Forschungsdimension ein und misst den Prozentsatz der Dokumente, die in Open-Access-Zeitschriften veröffentlicht wurden oder in Unpaywall indexiert sind. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das SCImago Institutions Ranking Unpaywall als Datenquelle [@SIR_method].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2025

**Website:** <https://www.scimagoir.com/>

**Ersteller:** [SCImago Research Group](https://www.scimagolab.com/)

**Ranking Typ:** Institutionenranking

**Sektoren:** Behörden, Hochschulen, Unternehmen, Gesundheitsorganisationen, Non-Profit Organisationen

**Link zur Methodik:** <https://www.scimagoir.com/methodology.php>

**Datenquellen:** Scopus (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Research (50%)

**Open Access Indikatoren (Gewichtung):** Prozentualer Anteil der Dokumente, die in Open-Access-Zeitschriften veröffentlicht oder in Unpaywall indexiert sind (2%)

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 9.756 Hochschulen und forschungsnahe Einrichtungen

**Geographische Abdeckung:** global

**Darstellung:** Rangliste (league table)

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website, keine Lizenzangabe: <https://www.scimagoir.com/rankings.php>
:::

### QS Sustainability Ranking 

Das QS World University Rankings: Sustainability wird seit 2023 jährlich von Quacquarelli Symonds erstellt, nachdem bereits 2022 eine Pilotversion veröffentlicht wurde [@QS_date]. Das Ranking bietet eine klassische Rangliste (league table) von Hochschulen anhand eines zusammengesetzten Indikators sowohl für die einzelnen Kategorien und Unterkategorien (*Lenses*), als auch eine Gesamtpunktzahl. Dabei werden die kategorialen Punktzahlen für alle bewerteten Institutionen angezeigt, während die Gesamtpunktzahlen nur bis zu einem bestimmten Punkt angezeigt und danach ausgeblendet werden [@QS_scores]. Das QS World University Rankings: Sustainability bewertet Institutionen entlang dreier Dimensionen: *Environmental Impact*, *Social Impact*, und *Governance*. 

Die Version 2025 des QS World University Rankings: Sustainability umfasst Daten für 1.744 Hochschulen weltweit [@QS_date]. Die bibliographischen Daten stammen aus Scopus und bilden einen 5-jahres Zeitraum ab [@QS_papers]. Es werden dabei ausschließlich die Publikationstypen Artikel, Review, Konferenzbeitrag, Buch, Buchkapitel, Article in Press und Business Article berücksichtigt [@QS_paperdef;@QS_scopus]. Institutionen werden nur in das QS World University Rankings: Sustainability aufgenommen, wenn sie einerseits Eignungskriterien für die Aufnahme in mindestens eines der Rankings *QS World University Rankings*, *QS Rankings by Region* oder *QS Rankings by Subject* erfüllen und zusätzlich noch eine Punktzahl ungleich Null für spezifische Indikatoren innerhalb der *Environmental Impact* und *Social Impact* erreichen [@QS_sustain].

Innerhalb der *Governance*-Dimension, die mit 10% in die Gesamtpunktzahl einfließt, bildet ein Indikator Open-Access-Publikationen ab. Dieser geht mit 1% Gewichtung in die *Governance*-Dimension ein und misst den Anteil an Open-Access-Publikationen am gesamten Publikationvolumen. Der Anteil wird einzeln für die sechs berücksichtigten Felder berechnet und zu einer gewichteten Summe aggregiert. Gold- und Hybrid-Publikationen werden bei der Berechnung stärker gewichtet als Publikationen, die über den grünen Open-Access-Weg veröffentlicht wurden. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das QS World University Rankings: Sustainability Unpaywall als Datenquelle [@QS_governance].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2025

**Website:** <https://www.topuniversities.com/sustainability-rankings>

**Ersteller:** [Quacquarelli Symonds Ltd.](https://www.qs.com/)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://support.qs.com/hc/en-gb/articles/8551503200668-QS-World-University-Rankings-Sustainability>

**Datenquellen:** Scopus (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Governance (10%)

**Open Access Indikatoren (Gewichtung):** Der Anteil der gesamten Forschungsergebnisse einer Einrichtung, die laut Unpaywall als Open Access verfügbar sind (1%)

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.744 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Rangliste (league table)

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website nach Registrierung, keine Lizenzangabe: <https://www.topuniversities.com/sustainability-rankings>

:::

### COKI OA Dashboard

Das COKI Open Access Dashboard wird seit 2022 von der Curtin Open Knowledge Intitiative erstellt [@COKI_date; @COKI_newsletter]. Das Dashboard ist kein klassisches Ranking, jedoch lassen sich Institutionen und Länder nach dem Open Access Anteil sowie den gesamten und Open-Access-Publikationsvolumina sortieren [@COKI_date]. Das Dashboard zeigt ferner für die einzelnen Länder und Institutionen an, über welche Plattform die Open-Access-Veröffentlichungen verfügbar gemacht werden, sowohl aggregiert über den gesamten Publikationszeitraum, als auch aufgeschlüsselt für die einzelnen Publikationsjahre [@COKI_date].

Die Version vom 18.08.2025 des COKI Open Access Dashboard umfasst Daten für 227 Länder und 56.187 Institutionen weltweit [@COKI_how]. Die bibliographischen Daten stammen aus mehreren offenen Datenquellen und berücksichtigen den Publikationszeitraum 2000--2024, zur Identifikation von Publikationen wird insbesondere Crossref genutzt [@COKI_how;@COKI_dashboard]. Die Auswahl der Publikationstypen ist im Vergleich zu den anderen Rankings weniger restriktiv, ausgeschlossen werden lediglich Publikationen mit den Typen Datensätze, Datenbanken, Komponenten, Berichtskomponenten, Peer Reviews, Fördermittel, Tagungsbände, Zeitschriftenausgaben, Berichtsreihen, Buchverzeichnisse, sowie alle Publikationen ohne angegebenen Publikationstyp [@COKI_how]. Institutionen müssen mindestens 50 Publikationen aufweisen, um in das COKI Open Access Dashboard aufgenommen zu werden, es sei denn, die Institutionen waren bereits im Datensatz aufgenommen, als dieser noch auf Microsoft Academic Graph basierte [@COKI_how].

Das COKI Open Access Dashboard legt den Fokus ausschließlich auf Open-Access-Aktivitäten der Institutionen und Länder. Während in der Listenansicht vorwiegend die absolute Anzahl sowie den prozentuale Anteil an Open-Access-Veröffentlichungen dargestellt wird, zeigen Detailansichten zu einzelnen Institutionen und Ländern differenziertere Aufschlüsselungen zu Publikationsaktivitäten und -orten, ergänzt durch Visualisierungen [@COKI_dashboard]. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das COKI Open Access Dashboard Unpaywall als Datenquelle [@COKI_how].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 18.08.2025

**Website:** <https://open.coki.ac/>

**Ersteller:** [Curtin Open Knowledge Initiative](https://openknowledge.community/)

**Ranking Typ:** Länder- und Institutionen-Ranking

**Sektoren:** Förderorganisationen, Behörden, Hochschulen, Unternehmen, Gesundheitsorganisationen, Non-Profit Organisationen, Infrastrukturorganisationen, Archive, Sonstige

**Link zur Methodik:** <https://open.coki.ac/how/>

**Datenquellen:** Crossref (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** --

**Open Access Indikatoren (Gewichtung):**

- Prozentualer Anteil an Open Access Veröffentlichungen
- Gesamtanzahl der Veröffentlichungen
- Gesamtanzahl an Open Access Veröffentlichungen
- Prozentualer Anteil an Artikeln, die über den Verlag Open Access zugänglich sind
- Prozentualer Anteil an Artikeln, die über andere Platformen Open Access zugänglich sind (Repositorien, Preprint-Server, etc.)
- Prozentualer Anteil an Artikeln, die sowohl über den Verlag, als auch über andere Platformen Open Access zugänglich sind
- Prozentualer Anteil an Closed Artikeln
- Prozentualer Anteil an Artikeln, die in einer Open Access Zeitschrift veröffentlicht wurden
- Prozentualer Anteil an Artikeln, die in einer Hybrid Zeitschrift
- Prozentualer Anteil an Artikeln, die über den Verlag ohne Wiederverwendungsrechte Open Access sind
- Prozentualer Anteil an Artikeln, die über institutionelle Repositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über Preprint Server Open Access sind
- Prozentualer Anteil an Artikeln, die über Fachrepositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über anderweitige, öffentliche Repositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über anderweitige Webseiten oder Portale Open Access sind

**Publikations-/Update-Rythmus:** regelmäßig

**Anzahl an Institutionen:** 56.187 Institutionen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Dashboard

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website unter CC-BY Lizenz: @COKI2025_07_21

:::


## Vergleichbarkeit der Ranking-Ergebnisse

Um zu beurteilen, wie vergleichbar die Ergebnisse der ausgewählten Rankings sind, konzentrieren sich die nachfolgenden Abschnitte auf die Überprüfung folgender Aspekte: die von den Rankings verwendeten Datenquellen, die von den Rankings verwendeten Open-Access-Indikatoren und deren Gewichtung, die Anzahl der in allen Rankings vertretenen Institutionen, die geografische Verteilung der Institutionen über Länder und Weltregionen hinweg sowie die Möglichkeit der institutionellen Beteiligung am Datenerhebungsprozess der Ranking-Anbieter.

Es soll dabei herausgearbeitet werden, inwiefern sich die einzelnen Rankingsysteme unterscheiden und welche Auswirkungen dies mitunter für die Nutzung eines bestimmten Rankings bei Open-Access-Monitoringaktivitäten oder der strategischen Entscheidungsfindung haben kann. 

### Datenquellen

In diesem Abschnitt werden die von den fünf Rankings verwendeten Datenquellen untersucht, wobei der Fokus auf folgenden zwei Kategorien von Datenquellen liegt: zum einen Datenquellen, die zur Erhebung bibliografischer Daten verwendet werden, und zum anderen Datenquellen, die zur Bestimmung des Open-Access-Status herangezogen werden. Unterschiede in der Abdeckung, Vollständigkeit und Methodik dieser Datenquellen können zu Abweichungen bei der Berechnung von Metriken führen und somit die Vergleichbarkeit der Ergebnisse unterschiedlicher Rankings einschränken. Ferner wird untersucht, ob sich aufgrund der verwendeten Datenquellen disziplinäre Unterschiede zwischen den Rankings ergeben.

#### Bibliographische Datenquellen

| Ranking |Web of Science | Scopus | OpenAlex | Crossref |
|:---------|:---------------:|:--------:|:----------:|:----------:|
| Leiden Ranking |  x | | | |
| Leiden Ranking Open Edition | | | x | |
| SCImago Institutions Ranking | | x | | |
| QS Sustainability Ranking | | x | | |
| COKI OA Dashboard | | | | x |

: Verwendete Datenquellen für die Erhebung bibliographischer Daten {#tbl-1}


#### Open-Access-Datenquellen

| Ranking | Unpaywall | OpenAlex |
|:--------|:---------:|:--------:|
| Leiden Ranking | | x |
| Leiden Ranking Open Edition | | x | 
| SCImago Institutions Ranking | x | |
| QS Sustainability Ranking | x | |
| COKI OA Dashboard | x | |

: Verwendete Datenquellen für die Bestimmung des Open-Access-Status {#tbl-2}


#### Disziplinäre Unterschiede

::: {.column-page}

::: {#fig-fields}

```{=html}
<iframe width="1040" height="560" src="https://app.powerbi.com/view?r=eyJrIjoiZmI0YzljYjYtYWZlNy00YzUzLWE1YjYtYjdkNmYwMDUzNjFmIiwidCI6ImNhMmE3Zjc2LWRiZDctNGVjMC05MTA4LTZiM2Q1MjRmYjdjOCIsImMiOjh9"></iframe>
```

Disziplinäre Unterschiede zwischen dem traditionellen Leiden Ranking 2024 und dem Leiden Ranking Open Edition 2024 (Quelle: [Open Dashboard 2024](https://app.powerbi.com/view?r=eyJrIjoiZmI0YzljYjYtYWZlNy00YzUzLWE1YjYtYjdkNmYwMDUzNjFmIiwidCI6ImNhMmE3Zjc2LWRiZDctNGVjMC05MTA4LTZiM2Q1MjRmYjdjOCIsImMiOjh9), [@LROE_resources])
:::

:::

### Open-Access-Indikatoren

Innerhalb der evaluativen Bibliometrie konzentrieren sich sogenannte Produktivitätsindikatoren auf die Auszählung von Publikationsvolumina, um die Produktivität von Einzelpersonen, Institutionen oder Ländern zu quantifizieren [@Gauch2023]. Open-Access-Indikatoren lassen sich in diesem Zusammenhang als eine Form von Produktivitätsindikatoren verstehen, da sie meist die Anzahl an Open-Access-Publikationen auszählen und daraus ein approximierendes Maß für die Produktivität von Forschenden, Institutionen oder Ländern im Bereich Open Access generieren. 

Aus einer guten bibliometrischen Praxis heraus, wie sie etwa im Leiden Manifest [@Hicks2015] umrissen ist, erfordert die Auswahl der im Kontext von Rankings verwendeten Indikatoren sorgfältige Überlegungen bezüglich deren Eignung für den Vergleich von Institutionen. Ein Vergleich zwischen Einrichtungen aus unterschiedlichen regionalen Kontexten oder mit unterschiedlicher Anzahl an Forschenden und Publikationsvolumina kann problematisch sein, wenn die Indikatoren lediglich absolute Anzahlen an Publikationen darstellen, da die spezifischen Bedingungen und Herausforderungen jeder Einrichtung nicht berücksichtigt werden. Um eine bessere Vergleichbarkeit zwischen Einrichtungen zu schaffen, finden daher bei der Indikatorenkonstruktion verschiedene Formen der Normalisierung Anwendung [@Gauch2023]. Eine Form der Normalisierung in Bezug auf Open-Access-Indikatoren kann sein, die absolute Anzahl der Open-Access-Publikationen einer Einrichtung ins Verhältnis zu setzen zur absoluten Anzahl aller Publikationen einer Einrichtung im untersuchten Zeitraum.

Neben einzelnen Indikatoren, werden auch sogenannte zusammengesetze Indikatoren (*composite indicators*) für einige Rankings verwendet, die als Rangliste angelegt sind. Diese Indikatoren werden aus gewichteten Aggregaten von einzelnen Indikatoren generiert [@Glaenzel2009;@Johnes2018]. Die Idee hinter zusammengesetzten Indikatoren besteht darin, komplexe Phänomene wissenschaftlicher Aktivitäten durch die Kombination und Verdichtung verschiedener Indikatoren zu einem einzigen, umfassenden Maß abzubilden. Durch diese Art der Linearisierung in eine einzelne Metrik, die sich sortieren lässt, ermöglichen zusammengesetze Indikatoren eine einfachere Vergleichbarkeit zwischen Einrichtungen, Personen oder Ländern [@quick_notes]. Dennoch gehen gewisse Problematiken mit zusammengesetzten Indikatoren einher, eine gute Übersicht findet sich bei @Forthmann2024. Zu den Problematiken und Kritikpunkten gehören beispielsweise, dass die Auswahl der Gewichtung einzelner Komponenten bei zusammengesetzen Indikatoren oft undurchsichtig und teils willkürlich erfolgt. Dabei haben die Gewichte der Einzelkomponenten einen beachtlichen Einfluss auf das resultierende Ranking [@Glaenzel2009;@Forthmann2024]. Ferner wird auch kritisiert, dass es oft an hinreichender Transparenz bezüglich der Indikatorenkonstruktion mangelt, was die Interpretation der Rankings erschweren kann [@Forthmann2024].

Beide Versionen des Leiden Rankings bieten im Kern zwei Arten von Open-Access-Indikatoren: 1) Indikatoren, die das Gesamtpublikationsvolumen an Open-Access-Publikationen einer Hochschule über einen vier-Jahreszeitraum ausweisen und 2) Indikatoren, die den Anteil des Open-Access-Publikationsvolumens am Gesamtpublikationsvolumen einer Hochschule über einen vier-Jahreszeitraum ausweisen. Beide Indikatoren werden einmal insgesamt für die Open-Access-Publikationen ausgewiesen und zusätzlich noch einmal separat für die gängigen Open-Access-Statustypen (gold, hybrid, grün, bronze, sowie für das traditionelle Leiden Ranking unknown). Neben den Open-Access-Indikatoren wird auch das Gesamtpublikationsvolumen pro Einrichtung angegeben. Auch wenn ein Vergleich zwischen Einrichtungen anhand absoluter Anzahlen Limitationen hat, so kann es doch sinnvoll sein, absolute Zahlen auszuweisen, um die Transparenz und Nachvollziehbarkeit der Indikatoren zu verbessern. Die Angabe der absoluten Anzahlen ermöglicht es den Einrichtungen, die Anteilswerte zu überprüfen und deren Genauigkeit zu beurteilen. 

Ähnlich wie die beiden Leiden Rankings, weist auch das COKI OA Dashboard sowohl absolute Anzahlen als auch relative Anteile in Bezug auf das Open-Access-Publikationsvolumen einer Einrichtung aus. Die im COKI OA Dashboard ausgewiesenen Indikatoren unterscheiden sich im Umfang zwischen der institutionellen Übersichtsseite und den Detailseiten der einzelnen Institutionen. Die Übersichtsseite, die einem Vergleich aller im COKI OA Dashboard repräsentierten Einrichtungen dient, weist dabei folgende Indikatoren für den Publikationszeitraum 2000--2024 aus: 1) den prozentualen Anteil an Open Access Veröffentlichungen, 2) ein gestapeltes Balkendiagramm welche die prozentualen Anteile an Artikeln visualisiert, die über den Verlag, andere Platformen, beides oder gar nicht im Open Access zugänglich sind. Das Balkendiagramm weist jedoch die jeweiligen Prozente nicht aus, 3) die Gesamtanzahl der Veröffentlichungen einer Einrichtung und 4) die Gesamtanzahl an Open Access Veröffentlichungen einer Einrichtung. Die Detailseiten der einzelnen Institutionen enthalten weitere spezifische Indikatoren, die eine detailliertere Übersicht des Open-Access-Publizierens einer Einrichtung ermöglichen. Insbesondere sind auf der Detailseite auch die Prozentangaben für das gestapelte Balkendiagramm angegeben und es werden unter anderem auch zeitliche Verläufe dargestellt. Auffällig ist, dass es eine Diskrepanz zwischen der Übersichtsseite und der Detailseite einer Einrichtung bezogen auf die absoluten Anzahlen für das Publikationsvolumen insgesamt und das Open-Access-Publikationsvolumen gibt. Auf der Übersichstsseite werden die exakten Werte angegeben, während auf der Detailsseite gerundete Werte dargestellt werden. Zudem handelt es sich bei dem Wert des prozentualen Open-Access-Anteils auf beiden Seiten um einen gerundeten Wert. Während die Angabe absoluter Zahlen, ähnlich zu den Leiden Rankings, für eine bessere Überprüfbarkeit der Indikatoren sorgt, ist die Darstellung gerundeter Werte weniger exakt.

Das SCImago Institutions Ranking verwendet einen zusammengesetzten Indikator, der verschiedene Indikatoren aus den drei Bereichen *Research*, *Innovation* und *Societal* aggregiert. Die *Research*-Dimension geht mit 50% Gewicht in den zusammengesetzen Indikator ein und ist gegenüber den beiden anderen Dimension mit 30% (*Innovation*) und 20% (*Societal*) Gewichtung demnach deutlich stärker fokussiert. Der Open-Access-Indikator ist im SCImago Institutions Ranking Teil der *Research*-Dimension und geht mit einem Gewicht von 2% in diese ein. Er gehört damit zu den am wenigsten stark gewichteten Indikatoren innerhalb der *Research*-Dimension und des zusammengesetzen Indikators insgesamt. Der Open-Access-Indikator gibt den prozentualen Anteil der Publikationen an, die in Open-Access-Zeitschriften veröffentlicht oder in Unpaywall indexiert sind [@SIR_method]. Das SCImago Institutions Ranking weist aber weder den Open-Access-Indikator noch die von den Einrichtungen erreichte Gesamtpunktzahl aus, sondern gibt lediglich den resultierenden Rang an.

Ähnlich wie das SCImago Institutions Ranking verwendet auch das QS Sustainability Ranking einen zusammengesetzten Indikator. Das QS Sustainability Ranking aggregiert dazu verschiedene Indikatoren aus den Bereichen *Environmental Impact*, *Social Impact*, und *Governance*. Die *Governance*-Dimension geht mit 10% Gewicht in den zusammengesetzen Indikator ein und ist damit im Gegensatz zur den Gewichtungn der beiden anderen Dimensionen mit jeweils 45 % deutlich weniger stark fokussiert. Der Open-Access-Indikator ist im QS Sustainability Ranking Teil der *Governance*-Dimension und geht mit einem Gewicht von 1% in diese ein. Alle Indikatoren innerhalb der *Governance*-Dimension haben das gleiche Gewicht zugewiesen, werden also gleichwertig für die Dimension berücksichtigt. Der Open-Access-Indikator gibt den Anteil der gesamten Forschungsergebnisse einer Einrichtung an, die laut Unpaywall als Open Access verfügbar sind [@QS_governance]. Bei der Indikatorenkonstruktion wird zwischen verschiedenen Open-Access-Statustypen (gold, hybrid, grün) und Feldern unterschieden, denen unterschiedliche Gewichtungen zugewiesen sind. Der finale Open-Access-Indikator wird anhand der einzelnen Werte pro Feld aggregiert und auf einen Wertebereich zwischen 0 und 100 skaliert [@QS_governance]. Der Datensatz QS Sustainability Ranking weist zusätzlich zur Gesamtpunktzahl für jede Dimension gesondert die erreichte Punktzahl aus. Die Gesamtpunktzahl wird dabei nur bis zum Rang 902 angegeben.

Insgesamt ist erkennbar, dass sich die fünf Rankingsysteme teilweise stark in der Verwendung von Open-Access-Indikatoren unterscheiden, auch wenn alle beschriebenen Open-Access-Indikatoren die Produktivität von Einrichtungen in Bezug auf Open Access messen und darstellen. Unterschiede und Herausforderungen für die Vergleichbarkeit der Rankingergebnisse ergeben sich beispielsweise aus der Berücksichtigung verschiedener Open-Access-Statustypen (gold, hybrid, grün, etc.) zur Klassifikation von Open-Access-Publikationen. Hierbei ergibt sich auch eine Pfadabhängigkeit zu der verwendeten Datenquelle, anhand derer die Open-Access-Publikationen identifiziert werden. Beide Versionen des Leiden Ranking verwenden OpenAlex als Datenquelle zur Identifikation von Open-Access-Publikationen und weisen ferner die Open-Access-indikatoren gesondert für alle Open-Access-Statustypen aus. Das QS Sustainability Ranking hingegen stützt sich auf Unpaywall als Datenquelle, scheint jedoch nur zwischen goldenen, hybriden und grünen Open-Access-Publikationen zu unterscheiden [@QS_governance]. Das SCImago Institutions Ranking verwendet Unpaywall als Datenquelle, scheint jedoch keine Open-Access-Statustypen zu unterscheiden [@SIR_method]. Das COKI OA Dashboard verwendet ebenfalls Unpaywall als Datenquelle zur Identifikation von Open-Access-Publikationen, weist aber im vergleich zu den anderen Ranking nicht die typischen Open-Access-Statustypen mit ihren Farben aus. Vielmehr werden im COKI OA Dashboard goldene, hybride und bronze Publikationen zu der Kategorie *Publisher Open* zusammengefasst. Diese Unterschiede erschweren die Vergleichbarkeit zwischen den Rankingsystemen in Bezug auf Open Access. 

Durch die differenziertere Darstellung der Indikatoren verlangen beide Versionen des Leiden Ranking sowie das COKI OA Dashboard  von den Nutzenden eine gesonderte Betrachtung der einzelnen Dimensionen, um eine umfassende Bewertung der Einrichtungen vornehmen zu können. Dies geht mit einer höheren Komplexität bei der Interpretation der Ergebnisse einher. Das SCImago Institutions Ranking und das QS SUstainability Ranking vereinfachen diese Komplexität durch die verwendeten zusammengesetzen Indikatoren. Allerdings erweisen sich die zusammengesetzen Indikatoren als weniger geeignet für das Open-Access-Monitoring und die Bewertung von Open-Access-Aktivitäten, da die spezifischen auf Open Access bezogenen Werte nicht direkt aus den zusammengesetzten Indikatoren abgeleitet werden können. Darüber hinaus spielen Open-Access-Indikatoren beim SCImago Institutions Ranking und dem QS Sustainability Ranking eine deutlich untergeordnete Rolle, da sie jeweils nur einen geringen Anteil an der Gesamtwertung ausmachen.

### Überschneidung von Institutionen

```{r}
#| label: loading-data
#| echo: false
#| warning: false
#| message: false

library(here)
library(tidyverse)
options(scipen = 999)

institutions_df_de_sector <- read_csv(here("rankings/data","institutions_df_de_sector.csv"))
institutions_df_de_univs <- read_csv(here("rankings/data", "institutions_df_de_univs.csv"))
pref_df <- read_csv(here("rankings/data", "pref_df.csv"))
```


Zur Bestimmung der institutionellen Überschneidungen zwischen den Rankings wurden zunächst die verfügbaren Ergebnisdaten der Rankings heruntergeladen[^1]. Neben den Namen und Standorten (Länderangaben, teilweise auch Angaben zu Regionen und Unterregion) aller Institutionen, enthalten die Daten der beiden Versionen des Leiden Rankings sowie des COKI OA Dashboards auch Identifier aus dem [Research Organization Registry (ROR)](https://ror.org/). Darüber hinaus umfassen die Daten des COKI OA Dashboard und des SCImago Institutions Ranking Sektorenzuordnungen für alle in den Rankings abgebildeten Institutionen. 

Um identische Institutionen zwischen den Rankings anhand ihrer ROR-IDs eindeutig zuordnen zu können, mussten zunächst die Daten des SCImago Institutions Rankings und des QS Sustainability Rankings um ROR-IDs ergänzt werden. Dazu wurde der gesamte ROR-Datensatz in der Version 1.69 [@ror_data] heruntergeladen. Anschließend wurde jeweils für beide Rankings ein maschineller Abgleich auf Basis der institutionellen Namensansetzung und der Länderangabe mit den ROR-Daten durchgeführt. Die QS Daten enthielten Einträge für insgesamt 1.743 Institutionen[^2], von denen 1.204 Institutionen mittels des maschinellen Abgleichs ROR-IDs zugeordnet werden konnten. Für die restlichen 539 Institutionen wurde eine manuelle Zuordnung der ROR-IDs vorgenommen, wobei allerdings für 7 Institutionen keine Zuordnung erfolgen konnte. Zur quantitativen Bestimmung der institutionellen Überschneidungen konnten demnach lediglich 1.736 Institutionen aus dem QS Sustainability Ranking berücksichtigt werden. 

Schwieriger gestaltete sich die Zuordnung von ROR-IDs zu den 9.755 im SCImago Institutions Ranking inkludierten Institutionen[^3]. Über den maschinellen Abgleich auf Basis der institutionellen Namensansetzung und der Länderangabe konnten ROR-IDs lediglich zu 4.714 Institutionen zugeordnet werden. In Teilen könnte dies darauf zurückzuführen sein, dass das SCImago Institutions Ranking auch Institutionen listet, die als multinational gekennzeichnet sind [@SIR_method]. Dies scheint insbesondere Unternehmen zu betreffen, die überregional agieren, beispielsweise [Google](https://www.scimagoir.com/institution.php?idp=89900). Zwar bietet ROR auch IDs für Unternehmen (siehe [Google](https://ror.org/00njsd438)), scheint dabei jedoch eindeutige Länderzuordnungen vorzunehmen. Die größere Schwierigkeit für den maschinellen Abgleich stellen jedoch vermutlich unterschiedliche Namensansetzungen zwischen dem SCImago Institutions Ranking und ROR dar[^4]. Um die Abdeckung mit ROR-IDs für das SCImago Institutions Ranking zu erhöhen, wurde ein fuzzy matching Ansatz getestet, der auch alternative Namensformen und Akronyme aus dem ROR-Datensatz berücksichtigte. Das Ergebnis war jedoch nicht hinreichend genau und hätte einen hohen Datenbereinigungsaufwand erfordert. Insgesamt stellt dies eine deutliche Limitation für die Verwendbarkeit des SCImago-Datensatzes für eine globale Analyse der institutionellen Überschneidungen dar. 

Aufgrund der Probleme bei der Zuordnung von ROR-IDs wurde entschieden, die Analyse der institutionellen Überschneidungen auf Einrichtungen aus Deutschland zu beschränken. Im Datensatz des QS Sustainability Ranking konnten insgesamt 50 Einrichtungen aus Deutschland identifiziert werden, von denen allen eine ROR-ID zugeordnet werden konnte. Im Datensatz des SCImago Institutions Ranking konnten insgesamt 364 Einrichtungen aus Deutschland identifiziert werden, von denen 342 keine ROR-ID über den maschinellen Abgleich zugeordnet werden konnten. Daher wurde eine manuelle Zuordnung vorgenommen, wobei jedoch für 6 der 342 Einrichtungen keine Zuordnung einer ROR-ID erfolgen konnte. Für die Analyse der institutionellen Überschneidungen deutscher Einrichtungen konnten demnach 385 Einrichtungen für das SCImago Institutions Ranking berücksichtigt werden.

@fig-2 stellt quantitativ die institutionellen Überschneidungen deutscher Einrichtungen zwischen den Rankings dar. Die Abbildung bezieht dabei nur die Institutionen ein, denen ROR-IDs zugeordnet werden konnten.

![Institutionelle Überschneidungen für alle deutschen Institutionen mit ROR-IDs in fünf Rankingsystemen](rankings/plots/all_de_institution_upset.png){#fig-2 group="upset"}

Insgesamt konnten nach dem Abgleich der ROR-IDs `r format(n_distinct(institutions_df_de_sector$ror), big.mark=".", decimal.mark = ",")` einzigartige Instiutionen aus Deutschland über alle Rankingsysteme hinweg identifiziert werden. Anhand von @fig-2 ist zu erkennen, dass das COKI OA Dashboard deutlich mehr deutsche Einrichtungen abbildet, als alle anderen Rankingsysteme, sowohl insgesamt (`r format(sum(institutions_df_de_sector$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(institutions_df_de_sector$coki) / n_distinct(institutions_df_de_sector$ror) *100, 2), decimal.mark=",")` %), als auch in Bezug auf die Anzahl an Institutionen, die in keinem anderen Rankingsystem abgebildet werden (`r format(sum(filter(institutions_df_de_sector, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(filter(institutions_df_de_sector, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki) / n_distinct(institutions_df_de_sector$ror) *100, 2), decimal.mark=",")` %). Dies könnte insbesondere darauf zurückzuführen sein, dass das COKI OA Dashboard einerseits einen relativ langen Publikationszeitraum abbildet (2000--2024) und andererseits auch Institutionen aus mehreren Sektoren, sowie Institutionen mit einem relativ geringen Publikationsvolumen von i.d.R. mindestens 50 Publikationen in das Ranking aufnimmt [@COKI_how;@COKI_data].

@fig-2 zeigt ferner, dass von den `r format(sum(institutions_df_de_sector$sir), big.mark=".", decimal.mark = ",")` aus dem SCImago Institutions Ranking berücksichtigten Institutionen, `r format(sum(filter(institutions_df_de_sector, sir == 1 & lr == 0 & lroe == 0 & qs == 0 & coki == 0)$sir), big.mark=".", decimal.mark = ",")` Institution (`r format(round(sum(filter(institutions_df_de_sector, sir == 1 & lr == 0 & lroe == 0 & qs == 0 & coki == 0)$sir) / n_distinct(institutions_df_de_sector$ror) *100, 2), decimal.mark=",")` %) in keinem anderen Ranking erfasst ist. Dahingegen werden alle Instiutionen aus beiden Versionen des Leiden Rankings sowie aus dem QS Sustainability Ranking auch in mindestens einem weiteren Ranking abgebildet. Insgesamt sind lediglich `r format(sum(filter(institutions_df_de_sector, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki), big.mark=".", decimal.mark = ",")` Institutionen (`r format(round(sum(filter(institutions_df_de_sector, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki) / n_distinct(institutions_df_de_sector$ror) *100, 2), decimal.mark=",")` %) in allen fünf Rankingsystemen erfasst. 

Während das QS Sustainability Ranking sowie beide Versionen des Leiden Ranking lediglich Hochschulen in ihre Rankings einbeziehen, sind im COKI OA Dashboard und dem SCImago Institutions Ranking auch Institutionen aus anderen Sektoren erfasst. Hochschulen werden im COKI-Datensatz mit dem Institutionentyp *Education* klassifiziert, während im Datensatz des SCImago Institutions Ranking der Sektor *Universities* für Hochschulen zugewiesen ist. Um folgend die institutionellen Überschneidungen der deutschen Hochschulen zwischen den fünf Rankingsystemen zu untersuchen, wurde im zusammengeführten Datensatz aller Rankingsysteme eine Spalte *sector* hinzugefügt. Für Einrichtungen, die in beiden Versionen des Leiden Ranking oder im QS Sustainability Ranking abgebildet sind, wurde die Sektorzuordnung *Universities* ergänzt. Für Einrichtungen, die im SCImago Institutions Ranking oder COKI OA Dashboard abgebildet sind wurde die ursprüngliche Klassifikation beibehalten. Anschließend wurde der Datensatz nach Einrichtungen gefiltert, denen in der *sector*-Spalte die Werte *Universities* oder *Education* zugewiesen waren.

@fig-3 stellt folgend die institutionellen Überschneidungen bezogen auf deutsche Hochschulen zwischen den Rankings dar. Die Abbildung bezieht dabei ebenfalls nur diejenigen Institutionen ein, denen ROR-IDs zugeordnet werden konnten.

![Institutionelle Überschneidungen für deutsche Hochschulen mit ROR-IDs in fünf Rankingsystemen](rankings/plots/univs_de_institution_upset.png){#fig-3 group="upset"}

Insgesamt konnten `r format(n_distinct(institutions_df_de_univs$ror), big.mark = ".")` verschiedene deutsche Hochschulen über alle Rankingsysteme hinweg identifiziert werden. @fig-3 zeigt, dass das COKI OA Dashboard auch in Bezug auf die deutschen Hochschulen deutlich mehr Institutionen abbildet, als alle anderen Rankingsysteme, sowohl insgesamt (`r format(sum(institutions_df_de_univs$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(institutions_df_de_univs$coki) / n_distinct(institutions_df_de_univs$ror) *100, 2), decimal.mark=",")` %), als auch in Bezug auf die Anzahl an Institutionen, die in keinem anderen Rankingsystem erfasst sind (`r format(sum(filter(institutions_df_de_univs, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(filter(institutions_df_de_univs, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki) / n_distinct(institutions_df_de_univs$ror) *100, 2), decimal.mark=",")` %). Nach Angaben des Statistischen Bundesamt wurden im Wintersemester 2024/2025 insgesamt 422 deutsche Hochschulen gezählt [@destatis]. Demnach bildet das COKI OA Dashboard nach aktuellem Stand `r format(round(n_distinct(institutions_df_de_univs$ror) / 422 * 100,2), decimal.mark = ",")` % aller deutschen Hochschulen ab. Es ist zudem das einzige Rankingsystem, dass Hochschulen abbildet, die in keinem anderen Ranking erfasst sind. Die absolute Anzahl an deutschen Hochschulen, die in allen fünf Rankingsystemen erfasst sind ist mit `r format(sum(filter(institutions_df_de_univs, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki), big.mark=".", decimal.mark = ",")` Hochschulen unverändert zur absoluten Anzahl aller deutschen Einrichtungen, die in allen fünf Rankingsystemen erfasst sind. 

Das COKI OA Dashboard und das SCImago Institutions Ranking sind die beiden einzigen der untersuchten Rankingsysteme, die nicht nur Hochschulen in ihre Rankings mit einbeziehen. In Bezug auf Deutschland entfallen auf die insgesamt `r format(sum(institutions_df_de_sector$coki), big.mark=".", decimal.mark = ",")` deutschen Einrichtungen, die im COKI OA Dashboard abgebildet sind `r format(round(sum(institutions_df_de_univs$coki) / sum(institutions_df_de_sector$coki) *100,2), big.mark=".", decimal.mark = ",")` % auf deutsche Hochschulen. Im Vergleich entfallen auf die insgesamt `r format(sum(institutions_df_de_sector$sir), big.mark=".", decimal.mark = ",")` deutschen Einrichtungen, die im SCImago Institutions Ranking abgebildet sind, `r format(round(sum(institutions_df_de_univs$sir) / sum(institutions_df_de_sector$sir) *100,2), big.mark=".", decimal.mark = ",")` % auf deutsche Hochschulen. Für beide Rankingsysteme lässt sich demnach feststellen, dass der Großteil an abgebildeten Einrichtungen, zumindest in Bezug auf Deutschland, anderen Sektoren zugewiesen ist. 


[^1]: Informationen zur jeweiligen Datenverfügbarkeit inklusive eines Links zum Download der verwendeten Version der Daten -- sofern vorhanden -- sind für alle Rankings in dem jeweiligen Kasten mit den Ranking Details erfasst.
[^2]: @QS_date berichtet, dass 1.744 Instiutionen in der Version 2025 geranked wurden. Die Daten zeigen hier jedoch eine Abweichung um eine Einrichtung.
[^3]: Die bereitgestellten Daten zeigen eine Abweichung um eine Institution zu der von SCImago auf der Rankingwebsite berichteten Anzahl an gerankten Institutionen [@SIR]. Die bereitgestellte csv-Datei enthält zwar 9.756 Zeilen, jedoch handelt es sich bei der ersten Zeile lediglich um eine Kopfzeile. 
[^4]: Beispielsweise wird im SCImago Institutions Ranking das [Deutsche Krebsforschungszentrum](https://www.scimagoir.com/institution.php?idp=21592) in der Landessprache angesetzt, während in ROR die englische Namensvariante [German Cancer Research Center](https://ror.org/04cdgtt98) als Hauptansetzung verwendet wird.

### Geographische Verteilung

Abbildungen -@fig-leidenmaps und -@fig-maps[^5] zeigen folgend die geographische Verteilung der Instiutionen in Bezug auf die Länder und Weltregionen, in denen sie lokalisiert sind. Auch wenn alle fünf Rankings eine globale Ausrichtung haben, soll anhand der Abbildungen visualisiert werden, ob es Unterschiede in der geographischen Verteilung zwischen den Rankings gibt. Um eine bessere Vergleichbarkeit zu gewährleisten wurden für alle Rankingsysteme die Instiutionen auf jene eingeschränkt, die dem Hochschulsektor (*Universities*, *Education*) zugeordnet sind.

::: {#fig-leidenmaps layout-ncol=2}


![Leiden Ranking 2024](rankings/plots/lr_map.png){#fig-LR group="map"}

![Leiden Ranking Open Edition 2024](rankings/plots/lroe_map.png){#fig-LROE group="map"}

Geographische Verteilung der in den Leiden Rankings abgebildeten Institutionen auf Länder und Weltregionen

:::

::: {#fig-maps layout-ncol=3}

![SCImago Institutions Ranking 2025 (Universities)](rankings/plots/sir_univ_map.png){#fig-sir group="map"}

![QS Sustainability Ranking 2025](rankings/plots/qs_map.png){#fig-qs group="map"}

![COKI OA Dashboard 2025 (Education)](rankings/plots/coki_univ_map.png){#fig-coki group="map"}

Geographische Verteilung der in den Rankingssystemen SIR, QS und COKI abgebildeten Institutionen auf Länder und Weltregionen

:::

Betrachtet man die absoluten Anzahlen an Hochschulen, so zeigen Abbildungen -@fig-leidenmaps und -@fig-maps insbesondere, dass für beide Versionen des Leiden Rankings sowie das SCImago Instiutions Ranking die meisten Hochschulen in China ansässig sind (LR: `r pref_df$nij_lr[pref_df$Country == "China"]` Institutionen, LROE: `r pref_df$nij_lroe[pref_df$Country == "China"]` Institutionen, SIR: `r pref_df$nij_sir[pref_df$Country == "China"]` Institutionen). Hingegen sind für das QS Sustainability Ranking sowie das COKI OA Dashboard die meisten Institutionen in den Vereinigten Staaten von Amerika ansässig (QS: `r pref_df$nij_qs[pref_df$Country == "United States"]` Institutionen, COKI: `r pref_df$nij_coki[pref_df$Country == "United States"]` Institutionen). Ferner verdeutlichen die Abbildungen, dass, mit Ausnahme des COKI OA Dashboards, insbesondere einige Länder aus dem globalen Süden gar nicht in den Rankingsystemen repräsentiert sind. 

Zusätzlich zur absoluten Anzahl an Hochschulen soll im Folgenden die relative Verteilung der Hochschulen über Länder und Rankingsysteme hinweg anhand der *preference* nach @Moed2016 betrachtet werden. @Moed2016 definiert die *preference* als[^6]:

> The preference of ranking system R for a particular country C is expressed as the ratio of the actual and the expected number of institutions from C appearing in R, where the expected number is based on the total number of institutions across countries and across systems, under the assumption of independence of these two variables. A value of 1.0 indicates that the number of institutions from C in R is ‘as expected’.


Der resultierende Präferenzwert (*preference score*) misst also, ob ein Land, gemessen an der Anzahl dort ansässiger Institutionen, innerhalb eines Rankingsystems überproportional oder unterproportional repräsentiert ist. Das Maß ist bei einem Wert von 1 neutral, d.h. die Anzahl an Institutionen aus einem Land in einem Rankingsystem entspricht den Erwartungen. Werte über 1 deuten darauf hin, dass mehr Institutionen aus einem Land in einem Rankingsystem erfasst sind als erwartet. Werte unter 1 deuten darauf hin, dass weniger Institutionen aus einem Land in einem Rankingsystem erfasst sind als erwartet. 


@tbl-pref stellt für alle fünf Rankingsysteme für die Top 5 Länder, gemessen am Präferenzwert (*preference score*), sowie Deutschland die absolute Anzahl an Hochschulen und den jweiligen Präferenzwert dar. 

::: {#tbl-pref layout-ncol=2}

+---------+-----------------------------+-----------------+------------------+
| Ranking | Land                        | Nr. Hochschulen | preference score |
+=========+=============================+=================+==================+
| Leiden  | Luxemburg                   | 1               |    2,45          |
|         +-----------------------------+-----------------+------------------+
|         | Australien                  | 35              |    2,34          |
|         +-----------------------------+-----------------+------------------+
|         | Spanien                     | 47              |    2,32          |
|         +-----------------------------+-----------------+------------------+
|         | Italien                     | 49              |    2,22          |
|         +-----------------------------+-----------------+------------------+
|         | China                       | 313             |    2,15          |
|         +-----------------------------+-----------------+------------------+
|         +-----------------------------+-----------------+------------------+
|         | Deutschland                 | 57              |    1,46          |
+---------+-----------------------------+-----------------+------------------+
| SIR     | Färöer-Inseln               | 1               |    2,55          |
|         +-----------------------------+-----------------+------------------+
|         | Französisch-Guayana         | 1               |    2,55          |
|         +-----------------------------+-----------------+------------------+
|         | Grenada                     | 1               |    2,55          |
|         +-----------------------------+-----------------+------------------+
|         | Guadeloupe                  | 1               |    2,55          |
|         +-----------------------------+-----------------+------------------+
|         | Lesotho                     | 1               |    2,55          |
|         +-----------------------------+-----------------+------------------+
|         +-----------------------------+-----------------+------------------+
|         | Deutschland                 | 108             |    0,82          |
+---------+-----------------------------+-----------------+------------------+
| COKI    | Myanmar                     | 27              |    1,61          |
|         +-----------------------------+-----------------+------------------+
|         | Kambodscha                  | 15              |    1,61          |
|         +-----------------------------+-----------------+------------------+
|         | Nicaragua                   | 15              |    1,61          |
|         +-----------------------------+-----------------+------------------+
|         | Somalia                     | 14              |    1,61          |
|         +-----------------------------+-----------------+------------------+
|         | El Salvador                 | 13              |    1,61          |
|         +-----------------------------+-----------------+------------------+
|         +-----------------------------+-----------------+------------------+
|         | Deutschland                 | 397             |    0,96          |
+---------+-----------------------------+-----------------+------------------+

+-----------+---------------+-----------------+------------------+
| Ranking   | Land          | Nr. Hochschulen | preference score |
+===========+===============+=================+==================+
| Leiden OE | Luxemburg     | 1               |    2,45          |
|           +---------------+-----------------+------------------+
|           | Australien    | 35              |    2,34          |
|           +---------------+-----------------+------------------+
|           | Spanien       | 47              |    2,32          |
|           +---------------+-----------------+------------------+
|           | Italien       | 49              |    2,22          |
|           +---------------+-----------------+------------------+
|           | China         | 313             |    2,15          |
|           +---------------+-----------------+------------------+
|           +---------------+-----------------+------------------+
|           | Deutschland   | 57              |    1,46          |
+-----------+---------------+-----------------+------------------+
| QS        | Brunei        | 2               |    3,70          |
|           +---------------+-----------------+------------------+
|           | Hongkong      | 9               |    3,17          |
|           +---------------+-----------------+------------------+
|           | Malta         | 1               |    2,96          |
|           +---------------+-----------------+------------------+
|           | Saudi-Arabien | 28              |    2,67          |
|           +---------------+-----------------+------------------+
|           | Jordanien     | 11              |    2,50          |
|           +---------------+-----------------+------------------+
|           +---------------+-----------------+------------------+
|           | Deutschland   | 50              |    1,11          |
+-----------+---------------+-----------------+------------------+


Präferenzwerte der Top 5 Länder und Deutschland pro Rankingsystem
:::

@tbl-pref zeigt unterschiedliche geografische Abdeckungen der verschiedenen Rankingsysteme in Bezug auf die überproportinal repräsentierten Länder auf. So sind beide Versionen des Leiden Ranking tendenziell auf Europa ausgerichtet, das SCImago Institutions Ranking tendenziell auf Nord- und Südamerika, das COKI OA Dashboard überwiegend auf Asien und Mittelamerika und das QS Sustainability Ranking primär auf Asien. Eine Besonderheit stellt das COKI OA Dashboard dar. Insgesamt weist der COKI Datensatz `r nrow(filter(pref_df,pref_coki == 1.61))` Länder mit dem selben Präferenzwert von 1,61 auf. Die in @tbl-pref dargestellten Länder sind jene ersten fünf Einträge, die sich aus einer absteigenden Sortierung nach der Anzahl an Institutionen aus den jeweiligen Ländern ergeben. Die Beobachtung, dass eine Vielzahl an Ländern den selben Präferenzwert haben ist darauf zurückzuführen, dass die `r nrow(filter(pref_df,pref_coki == 1.61))` Länder ausschließlich im COKI OA Dashboard enthalten sind.

Bezogen auf Deutschland zeigt @tbl-pref, dass beiden Versionen des Leiden Ranking sowie das QS Sustainability Ranking mit Präferenzwerten von 1,46 und 1,11 eine überproportionale bzw. leicht überproportionale Anzahl an deutschen Hochschulen in ihre Rankings einbeziehen. Dies deutet darauf hin, dass in diesen Rankingsystemen deutsche Hochschulen mit einer absoluten Anzahl von 50 und 57 Hochschulen leicht überrepräsentiert bis überrepräsentiert sind. Hingegen sind im COKI OA Dashboard mit einer absoluten Anzahl von 397 zwar wesentlich mehr deutsche Hochschulen im Ranking enthalten, allerdings zeigt der Präferenzwert von 0.96, dass diese Anzahl unter dem Erwartungswert liegt. Für das SCImago Institutions Ranking beträgt der Präferenzwert für deutsche Hochschulen lediglich 0,82. Dies deutet darauf hin, dass Einrichtungen aus Deutschland in diesem Ranking unterrepräsentiert sind, obwohl die absolute Anzahl an deutschen Hochschulen mit 108 die Zweithöchste ist.


[^5]: Durch Klick auf die Abbildungen werden diese vergrößert dargestellt.
[^6]: Formal definiert @Moed2016 die *preference* mit: $P= \dfrac{\bigg(\dfrac{n[i, j]}{\sum_i n[i, j]}\bigg)}{\bigg(\dfrac{\sum_jn[i, j]}{\sum_i\sum_jn[i, j]}\bigg)}$, wobei $n[i, j]$ die Anzahl an Institutionen aus Land $i$ im Rankingsystem $j$, $\sum_in[i, j]$ die Summe aller $n[i, j]$ über alle $i$ (Länder), $\sum_jn[i, j]$ die Summe aller $n[i, j]$ über alle $j$ (Rankingsysteme), und $\sum_i\sum_jn[i, j]$ die Summe aller $n[i, j]$ über alle $i$ (Länder) und alle $j$ (Rankingsysteme) angibt.

### Institutionelle Beteiligung bei der Datenerhebung

Dieser Abschnitt befasst sich mit den Datenerhebungsverfahren der verschiedenen Rankinganbieter und untersucht inwieweit sich Einrichtungen an der sie betreffenden Datenerhebung beteiligen können.

Für das traditionelle Leiden Ranking erhebt das CWTS keinerlei Daten direkt von den gerankten Einrichtungen [@LR_method;@LR_general]. Das Ranking basiert auf Daten aus dem Web of Science, die vom CWTS aufbereitet und standardisiert werden. Da es sich beim Web of Science jedoch um eine proprietäre Datenquelle handelt, die mit lizenzrechtlichen Restriktionen einhergeht, ist es dem CWTS nicht möglich die Rohdaten sowie die kuratierten Daten offen zugänglich zu machen [@LeidenData]. Dadurch ist es den betreffenden Einrichtungen nicht ohne Weiteres möglich die Datenqualität zu überprüfen. 

Für das Leiden Ranking Open Edition werden ebenfalls keine Daten direkt bei den Einrichtungen erhoben [@LROE_method]. Im Gegensatz zum traditionellen Leiden Ranking greift das Leiden Ranking Open Edition jedoch auf offene Datenquellen, wie OpenAlex und ROR, zur Erstellung des Rankings zurück [@LROE_method]. Durch die Verwendung von offenen Datenquellen und aufgrund der Bereitstellung der Roh- und Ergebnisdaten durch das CWTS [@LROE_resources] ist es den gerankten Einrichtungen möglich, einerseits die Qualität der Daten zu überprüfen und andererseits am Datenkurierungsprozess teilzunehmen, indem die Einrichtungen Probleme mit der Datenqualität an die jeweiligen Akteure melden [@LeidenData].

Die Methodik des SCImago Institutions Rankings beschreibt, dass für das Ranking Daten aus verschiedenen Quellen erhoben werden, dies sind: Scopus, PATSTAT, Google, Semrush, Unpaywall, PlumX Metrics, Mendeley und Overton [@SIR_method]. Die Methodik erwähnt keine Möglichkeit der institutionellen Beteiligung bei der Datenerhebung, weshalb davon ausgegangen wird, dass keine Daten direkt von den gerankten Institutionen erhoben werden. Die bibliographischen Daten für das SCImago Institutions Ranking stammen aus Scopus, einer proprietären Datenquelle, die, ähnlich wie das Web of Science, mit lizenzrechtlichen Einschränkungen einhergeht. Die Rohdaten des SCImago Institutions Ranking sind nicht öffentlich zugänglich, wodurch davon auszugehen ist, dass gerankte Institutionen nicht ohne Weiteres die Möglichkeit haben, die sie betreffenden Daten zu überprüfen oder zu korrigieren, bevor sie in das Ranking einbezogen werden. Zur Bestimmung des Open-Access-Status verwendet das SCImago Institutions Ranking mit Unpaywall eine offene Datenquelle. Ähnlich wie beim Leiden Ranking Open Edition könnte für die Open-Access-Daten eine Beteiligung der betreffenden Einrichtungen am Datenkuratierungsprozess möglich sein, indem Probleme mit der Datenqualität direkt an Unpaywall gemeldet werden. Insgesamt weist die Methodik des SCImago Institutions Ranking wenig Informationen bezüglich Datenqualität und mögliche Einschränkungen auf, es wird auch nicht transparent kommuniziert, welche Möglichkeiten es für Einrichtungen gibt zu einer verbesserten Datenqualität beizutragen.

Die COKI Open Access Dashboard verwendet zur Datenerhebung ausschließlich offene Datenquellen, darunter Crossref, Unpaywall, oder ROR [@COKI_how]. Die Methodik des COKI OA Dashboard beschreibt detailliert den Prozess der Datenerhebung anhand externer Datenquellen, macht jedoch keine Angaben zu einer Datenlieferung durch die gerankten Institutionen selbst [@COKI_how]. Es ist daher davon auszugehen, dass es keine direkte institutionelle Beteiligung bei der Datenerhebung gibt. Da der COKI-Datensatz auf offenen Datenquellen basiert, ist ähnlich wie beim Leiden Ranking Open Edition davon auszugehen, dass die betreffenden Institutionen die Möglichkeit haben sich indirekt am Datenkuratierungsprozess zu beteiligen, indem sie Probleme mit der Datenqualität direkt an die jeweiligen Verantwortlichen der verschiedenen Datenquellen melden. Darüber hinaus kommuniziert COKI Limitationen des eigenen Datensatzes [@COKI_how] und bietet verschiedene Kontaktmöglichkeiten [@COKI_contact], sodass Einrichtungen auch die Möglichkeit haben, aktiv an der Verbesserung der Daten des COKI-Datensatzes mitzuwirken.

Das QS Sustainability Ranking ist das einzige der fünf untersuchten Rankingsysteme, dass eine direkte Beteiligung der betreffenden Institutionen bei der Datenerhebung vorsieht. Die Methodik des QS Sustainability Ranking beschreibt, dass QS generell alle Institutionen ermutigt, Daten für die QS Rankings bereitzustellen [@QS_sustain]. Zur Datenübermittlung stellt QS eine spezielle Plattform bereit, die Methodik bietet jedoch keine genaue Auflistung, welche Daten erfragt werden [@QS_sustain]. Sollten Institutionen, die für die Aufnahme in das QS Sustainability Ranking qualifiziert sind keine Daten liefern, so bewertet QS diese Institutionen auf Grundlage von proprietären und öffentlich verfügbaren Daten oder zuvor von der jeweiligen Einrichtung bereitgestellten Daten. Ein gänzliches opt-out aus dem Ranking von institutioneller Seite ist demnach nicht vorgesehen. QS argumentiert dies damit, dass die Inklusion aller qualifizierenden Einrichtungen die Zuverlässigkeit der Rankings erhöht [@QS_optout]. Allerdings bietet QS keine Angaben dazu, wie viele der gerankten Einrichtungen tatsächlich die Möglichkeit zur Datenlieferung wahrnehmen.

Insgesamt zeigt sich, dass die Möglichkeit für Einrichtungen, sich an der sie betreffenden Datenerhebung der Rankinganbieter zu beteiligen, nur begrenzt gegeben ist. Das QS Sustainability Ranking ist das einzige Rankingsystem, das direkt Daten von den betreffenden Institutionen erhebt, wobei nicht ganz klar ist, welche spezifischen Daten von den Einrichtungen abgefragt werden. Die anderen Rankingsysteme verwenden ausschließlich Daten aus externen Quellen, die nicht direkt von den Institutionen erhoben werden. Die Verwendung von offenen Datenquellen, wie im Leiden Ranking Open Edition oder dem COKI OA Dashboard, kann jedoch den Einrichtungen eine Möglichkeit bieten, die Qualität der sie betreffenden Daten zu verbessern. Qualitativ hochwertige Quelldaten, sowie transparente Methodiken für die Erstellung von Rankings können ferner dazu beitragen das Vertrauen in Verlässlichkeit und Aussagekraft der Rankingsysteme zu verbessern.