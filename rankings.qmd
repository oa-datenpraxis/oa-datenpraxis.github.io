---
title: "Open Access in institutionellen Rankings"
format: 
  html: 
    resources: 
      - rankings/data/shinylive-sw.js
      - rankings/data/leiden_shiny.csv
filters:
  - shinylive
bibliography: rankings/data/rankings.bib
csl: apa.csl
lightbox:
  match: auto
crossref:
  fig-title: "Abbildung"
  fig-prefix: "Abb."
  tbl-title: "Tabelle"
  tbl-prefix: "Tab."
---

Institutionelle Rankings oder Hochschulrankings genießen eine hohe Aufmerksamkeit in Wissenschaftspolitik und Öffentlichkeit, weil es ihnen gelingt, wissenschaftliche Einrichtungen auf globaler Ebene übersichtlich miteinander zu vergleichen. Sie dienen verschiedenen Interessengruppen, wie politischen Entscheidungsträgern, Hochschulmanagern oder Studieninteressierten, unter anderem als Instrument zur strategischen Entscheidungsfindung, Veranschaulichung der institutionellen Exzellenz oder bei der Wahl des Studienorts [@Waltman2012]. Die jeweilige Methodik, auf der die Rankings basieren, ist mitunter vielzähliger Kritikpunkte ausgesetzt, beispielsweise wegen ihrer Tendenz, mehrere Dimensionen des Leistungsspektrums wissenschaftlicher Einrichtungen zu einer einzigen, oft undurchsichtigen Punktzahl zusammenzufassen [@vanRaan2005;@Diprose2023]. Dieser Ansatz erschwert nicht nur die Interpretation der Rankings, sondern berücksichtigt auch nicht hinreichend die unterschiedlichen Aufgaben und Prioritäten der Einrichtungen [@Waltman2012;@Huang2020].

In den letzten Jahren hat die Einbeziehung von Open-Access-Kennzahlen in Hochschulrankings an Bedeutung gewonnen. Open Access bezeichnet den freien und uneingeschränkten Online-Zugang zu Forschungsergebnissen [@Suber2012] und ist zu einem wichtigen Indikator für das Engagement einer wissenschaftlichen Einrichtung für eine offene und transparante Wissenschaftspraxis geworden. Diese Entwicklung wird zusätzlich durch die Vorgaben von Förderorganisationen unterstützt, die Open Access zunehmend als Voraussetzung für die Forschungsförderung betrachten [@Wellcome_Trust;@NIH;@DFG;@Fournier2017;@Huang2020a;@Ruecknagel2021]. Diese Vorgaben setzen neben dem Selbstverständnis vieler Einrichtungen einen zusätzlichen Anreiz Forschungsergebnisse im Open Access zugänglich zu machen. Die Integration von Open-Access-Kennzahlen in Hochschulrankings ermöglicht den Aktivitäten der Einrichtungen im Bereich Open Access Rechnung zu tragen und bietet eine zusätzliche Vergleichsdimension [@Diprose2023].

Es konnten fünf institutionelle Rankings mit Open Access als Vergleichsdimension identifiziert werden: das Leiden Ranking 2024 (LR), das Leiden Ranking Open Edition 2024 (LROE), das SCImago Institutions Ranking 2024 (SIR), das Quacquarelli Symonds World University Rankings: Sustainability 2025 (QS) und das COKI OA Dashboard 2025 (COKI). Alle fünf Rankings enthalten einen oder mehrere Open-Access-Indikatoren und sind hinsichtlich ihrer geografischen Abdeckung global ausgerichtet. 

Diese Seite bietet eine Übersicht über die Charakteristika der fünf Rankings, sowie eine Untersuchung dazu, inwieweit die Ergebnisse der Rankings miteinander vergleichbar sind. Dies beinhaltet unter anderem einen Vergleich der von den Rankings verwendeten Datenquellen und Open-Access-Indikatoren, sowie eine Erhebung dazu, in welchem Umfang sich die Einrichtungen an der Datenerhebung durch die Rankinganbieter beteiligen können. Der Fokus liegt dabei insbesondere auf der Open-Access-Dimension.


## Institutionellen Rankings mit Open Access als Vergleichsdimension

Die folgenden Abschnitte beginnen mit einer Beschreibung der Kernmerkmale der ausgewählten Rankings. Am Ende jedes Abschnitts werden Details zu den Rankings in einem aufklappbaren Textkasten mit der Überschrift „Ranking-Details“ zusammengefasst. Dazu gehört insbesondere eine umfassende Liste der in jedem Ranking verwendeten Open-Access-Indikatoren. 


### Leiden Ranking

Das Leiden Ranking wird seit 2007 jährlich vom Centre for Science and Technology Studies (CWTS) der Universität Leiden in den Niederlanden erstellt [@Waltman2012;@LR_updates]. Das Ranking bietet keine klassische Rangliste, bei der eine Gesamtkennzahl berechnet wird, jedoch verschiedene bibliometrische Indikatoren entlang der Dimensionen *scientific impact*, *collaboration*, *Open Access* und *gender*. Die Indikatoren werden sowohl in größenabhängiger (absolute Zahlen) als auch in größenunabhängiger (relative Anteile) Form dargestellt, um Unterschiede in der Größe der Hochschulen zu berücksichtigen. Das Ranking bietet auch Trendanalysen über Zeiträume und Stabilitätsintervalle, um statistische Unsicherheiten zu berücksichtigen [@LR_general;@LR_indicators]. 

Die Version 2024 des Leiden Ranking umfasst bibliometrische Daten für 1.506 Hochschulen weltweit [@LR_universities]. Indikatoren werden jeweils für Zeiträume von vier Jahren angegeben, die Version 2024 erweitert diese um den Zeitraum 2019--2024 [@LR_indicators]. Die bibliographischen Daten stammen aus dem Web of Science, wobei jedoch nur Artikel und Reviews berücksichtigt werden, die in internationalen wissenschaftlichen Zeitschriften veröffentlicht wurden [@LR_indicators]. Die Auswahl der Hochschulen erfolgt auf der Grundlage ihrer Forschungsleistung, wobei diese mindestens 800 sogenannte *core publications* vorweisen müssen, um in das Ranking aufgenommen zu werden. Unter *core publications* werden Veröffentlichungen verstanden, die in internationalen wissenschaftlichen Zeitschriften in solchen Feldern veröffentlicht wurden, die für die Zitationsanalyse geeignet sind. Diese Publikationen müssen in englischer Sprache verfasst sein, mindestens eine:n Autor:in haben, dürfen nicht zurückgezogen (*retracted*) worden sein und müssen in einem sogenannten *core journal* erschienen sein. *core journals* zeichnen sich durch ihre internationale Ausrichtung aus und müssen in einem Feld angesiedelt sein, das für die Zitationsanalyse geeignet ist. Das Leiden Ranking verwendet keine Daten, die direkt von Hochschulen stammen [@LR_data;@LR_universities;@LR_indicators;@LR_general].

Ein wesentliches Merkmal des Leiden Ranking ist der Fokus auf Open-Access-Veröffentlichungen. Mit der Version 2019 des Rankings wurden Indikatoren zum Open-Access-Publizieren hinzugefügt [@LR_updates]. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das Leiden Ranking OpenAlex als Datenquelle [@LR_data; @LR_indicators;@LR_general].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2024

**Website:** <https://www.leidenranking.com/>

**Ersteller:** [Centre for Science and Technology Studies (CWTS)](https://www.universiteitleiden.nl/en/social-behavioural-sciences/cwts)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://www.leidenranking.com/information/general>

**Datenquellen:** Web of Science (Publikationen), OpenAlex (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Open Access

**Open Access Indikatoren (Gewichtung):**

- Gesamtzahl der Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Open-Access-Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Gold-Open-Access-Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Hybrid-Open-Access-Publikationen einer Hochschule
- Die Anzahl und der Anteil der Bronze-Open-Access-Publikationen einer Hochschule
- Die Anzahl und der Anteil der grünen Open-Access-Publikationen einer Hochschule
- Die Anzahl und der Anteil der Publikationen einer Hochschule, deren Open-Access-Status unbekannt ist

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.506 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Ranking

**Datenverfügbarkeit:** Ergebnisdaten verfügbar unter CC-BY Lizenz: @LR2024_data

:::


### Leiden Ranking Open Edition


Das Leiden Ranking Open Edition wird seit 2024 [@LROE2024] jährlich vom Centre for Science and Technology Studies (CWTS) der Universität Leiden in den Niederlanden erstellt. Es ist eine annähernde Reproduktion des traditionellen Leiden Ranking unter Nutzung offener Datenquellen [@LROE2024] und bietet ebenfalls keine klassische Rangliste, sondern verschiedene bibliometrische Indikatoren entlang der Dimensionen *scientific impact*, *collaboration*, und *Open Access*. Die Indikatoren werden in der Open Edition ebenfalls sowohl in größenabhängiger (absolute Zahlen) als auch in größenunabhängiger (relative Anteile) Form dargestellt, um Unterschiede in der Größe der Hochschulen zu berücksichtigen. Die Open Edition des Leiden Ranking bietet zudem ebenfalls Trendanalysen über Zeiträume und Stabilitätsintervalle, um statistische Unsicherheiten zu berücksichtigen [@LROE_indicators;@LROE_updates]. 

Die Version 2024 des Leiden Ranking umfasst bibliometrische Daten für 1.506 Hochschulen weltweit [@LROE_universities]. Indikatoren werden jeweils für Zeiträume von vier Jahren angegeben, die Version 2024 erweitert diese um den Zeitraum 2019--2024 [@LROE_indicators]. Die bibliographischen Daten stammen aus OpenAlex, wobei jedoch nur sogenannte *core publications* berücksichtigt werden [@LROE_indicators]. Im Vergleich zum traditionellen Leiden Ranking zählen in der Open Edition zusätzlich Publikationen vom Typ Buchkapitel, die in Buchreihen veröffentlicht wurden zu den *core publications*. Ferner müssen alle Publikationen nicht nur Autor:innenangaben, sondern auch Affiliationsangaben und Referenzen enthalten, um zu den *core publications* zu zählen. Alle anderen Kriterien für die *core publications* und *core journals* sind deckungsgleich zum traditionellen Leiden Ranking [@LROE_indicators].

Auch das Leiden Ranking Open Edition legt einen Fokus auf Open-Access-Veröffentlichungen und bietet weitestgehend die selben Open-Access-Indikatoren wie das traditionelle Leiden Ranking, mit Ausnahme der Indikatoren zu Publikationen mit unklarem Open-Access-Status [@LROE_indicators]. Darüber hinaus weist das Leiden Ranking Open Edition die Besonderheit auf, dass auf der Weboberfläche die den Indikatoren zugrunde liegenden Publikationen einsehen werden können [@LROE2024a].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2024

**Website:** <https://www.open.leidenranking.com/>

**Ersteller:** [Centre for Science and Technology Studies (CWTS)](https://www.universiteitleiden.nl/en/social-behavioural-sciences/cwts)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://open.leidenranking.com/information/universities>

**Datenquellen:** OpenAlex

**Dimension mit Open Access Indikatoren (Gewichtung):** Open Access

**Open Access Indikatoren (Gewichtung):**

- Gesamtzahl der Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Open-Access-Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Gold-Open-Access-Veröffentlichungen einer Hochschule
- Anzahl und Anteil der Hybrid-Open-Access-Publikationen einer Hochschule
- Die Anzahl und der Anteil der Bronze-Open-Access-Publikationen einer Hochschule
- Die Anzahl und der Anteil der grünen Open-Access-Publikationen einer Hochschule

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.506 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Ranking

**Datenverfügbarkeit:** Rohdaten und Ergebnisdaten verfügbar unter CC0 Lizenz: 

- Rohdaten: @LROE2024_raw
- Ergebnisdaten: @LROE2024_results

:::

### SCImago Institutions Ranking

Das SCImago Institutions Ranking wird seit 2009 jährlich von der SCImago Research Group erstellt [@Rousseau2018]. Das Ranking bietet eine klassische Rangliste (league table) von wissenschaftlichen und forschungsnahen Einrichtungen anhand eines zusammengesetzten Indikators auf einer Skala zwischen 0 und 100. Der zusammengesetzte Indikator kombiniert Bewertungen entlang dreier Dimensionen: Forschung, Innovation und gesellschaftliche Wirkung. Der zusammengesetzte Indikator ergibt sich aus einer gewichtete Kombination von Metriken entlang dieser Dimensionen, wobei sowohl größenabhängige als auch größenunabhängige Metriken verwendet werden. Für jede Einrichtung bietet das Ranking zusätzlich noch eine Reihe von Grafiken beispielsweise zu der Rangentwicklung über die Zeit, Vergleichen auf nationaler, regionaler oder globaler Ebene, oder ähnlichen Institutionen gemäß des Publikationsprofils [@SIR_method].

Die Version 2025 des SCImago Institutions Ranking umfasst Daten für 9.756 Institutionen weltweit [@SIR]. Der zusammengesetzte Indikator wird pro Jahr für einen Zeitraum von fünf Jahren bis zwei Jahre vor Veröffentlichung des Rankings angegeben, für die Version 2025 also für den Zeitraum 2019--2023. Die bibliographischen Daten stammen aus Scopus. Die Auswahl der Institutionen erfolgt auf der Grundlage ihrer Veröffentlichungsleistung. Im letzten Jahr des betrachteten Zeitraums müssen die Institutionen mindestens 100 in Scopus indexierte Publikationen veröffentlicht haben, um in das Ranking aufgenommen zu werden. Ferner müssen mindestens 75% des gesamten Publikationsvolumens einer Einrichtung sogenannte zitierfähige Dokumente ausmachen. Zu den zitierfähigen Dokumenten werden für das SCImago Institutions Ranking die Publikationstypen Artikel, Buchkapitel, Konferenzbeiträge, Reviews und Short Surveys gezählt [@SIR_method].

Innerhalb der Forschungsdimension, die mit 50% in den zusammengesetzten Indikator einfließt, wurde mit der Version 2019 ein Open-Access-Indikator dem Ranking hinzugefügt. Dieser geht mit 2% Gewichtung in die Forschungsdimension ein und misst den Prozentsatz der Dokumente, die in Open-Access-Zeitschriften veröffentlicht wurden oder in Unpaywall indexiert sind. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das SCImago Institutions Ranking Unpaywall als Datenquelle [@SIR_method].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2025

**Website:** <https://www.scimagoir.com/>

**Ersteller:** [SCImago Research Group](https://www.scimagolab.com/)

**Ranking Typ:** Institutionenranking

**Sektoren:** Behörden, Hochschulen, Unternehmen, Gesundheitsorganisationen, Non-Profit Organisationen

**Link zur Methodik:** <https://www.scimagoir.com/methodology.php>

**Datenquellen:** Scopus (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Research (50%)

**Open Access Indikatoren (Gewichtung):** Prozentualer Anteil der Dokumente, die in Open-Access-Zeitschriften veröffentlicht oder in Unpaywall indexiert sind (2%)

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 9.756 Hochschulen und forschungsnahe Einrichtungen

**Geographische Abdeckung:** global

**Darstellung:** Rangliste (league table)

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website, keine Lizenzangabe: <https://www.scimagoir.com/rankings.php>
:::

### QS Sustainability Ranking 

Das QS World University Rankings: Sustainability wird seit 2023 jährlich von Quacquarelli Symonds erstellt, nachdem bereits 2022 eine Pilotversion veröffentlicht wurde [@QS_date]. Das Ranking bietet eine klassische Rangliste (league table) von Hochschulen anhand eines zusammengesetzten Indikators sowohl für die einzelnen Kategorien und Unterkategorien (*Lenses*), als auch eine Gesamtpunktzahl. Dabei werden die kategorialen Punktzahlen für alle bewerteten Institutionen angezeigt, während die Gesamtpunktzahlen nur bis zu einem bestimmten Punkt angezeigt und danach ausgeblendet werden [@QS_scores]. Das QS World University Rankings: Sustainability bewertet Institutionen entlang dreier Dimensionen: *Environmental Impact*, *Social Impact*, und *Governance*. 

Die Version 2025 des QS World University Rankings: Sustainability umfasst Daten für 1.744 Hochschulen weltweit [@QS_date]. Die bibliographischen Daten stammen aus Scopus und bilden einen 5-jahres Zeitraum ab [@QS_papers]. Es werden dabei ausschließlich die Publikationstypen Artikel, Review, Konferenzbeitrag, Buch, Buchkapitel, Article in Press und Business Article berücksichtigt [@QS_paperdef;@QS_scopus]. Institutionen werden nur in das QS World University Rankings: Sustainability aufgenommen, wenn sie einerseits Eignungskriterien für die Aufnahme in mindestens eines der Rankings *QS World University Rankings*, *QS Rankings by Region* oder *QS Rankings by Subject* erfüllen und zusätzlich noch eine Punktzahl ungleich Null für spezifische Indikatoren innerhalb der *Environmental Impact* und *Social Impact* erreichen [@QS_sustain].

Innerhalb der *Governance*-Dimension, die mit 10% in die Gesamtpunktzahl einfließt, bildet ein Indikator Open-Access-Publikationen ab. Dieser geht mit 1% Gewichtung in die *Governance*-Dimension ein und misst den Anteil an Open-Access-Publikationen am gesamten Publikationvolumen. Der Anteil wird einzeln für die sechs berücksichtigten Felder berechnet und zu einer gewichteten Summe aggregiert. Gold- und Hybrid-Publikationen werden bei der Berechnung stärker gewichtet als Publikationen, die über den grünen Open-Access-Weg veröffentlicht wurden. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das QS World University Rankings: Sustainability Unpaywall als Datenquelle [@QS_governance].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 2025

**Website:** <https://www.topuniversities.com/sustainability-rankings>

**Ersteller:** [Quacquarelli Symonds Ltd.](https://www.qs.com/)

**Ranking Typ:** Hochschulranking

**Sektoren:** Hochschulen

**Link zur Methodik:** <https://support.qs.com/hc/en-gb/articles/8551503200668-QS-World-University-Rankings-Sustainability>

**Datenquellen:** Scopus (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** Governance (10%)

**Open Access Indikatoren (Gewichtung):** Der Anteil der gesamten Forschungsergebnisse einer Einrichtung, die laut Unpaywall als Open Access verfügbar sind (1%)

**Publikations-/Update-Rythmus:** jährlich

**Anzahl an Institutionen:** 1.744 Hochschulen

**Geographische Abdeckung:** global

**Darstellung:** Rangliste (league table)

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website nach Registrierung, keine Lizenzangabe: <https://www.topuniversities.com/sustainability-rankings>

:::

### COKI OA Dashboard

Das COKI Open Access Dashboard wird seit 2022 von der Curtin Open Knowledge Intitiative erstellt [@COKI_date; @COKI_newsletter]. Das Dashboard ist kein klassisches Ranking, jedoch lassen sich Institutionen und Länder nach dem Open Access Anteil sowie den gesamten und Open-Access-Publikationsvolumina sortieren [@COKI_date]. Das Dashboard zeigt ferner für die einzelnen Länder und Institutionen an, über welche Plattform die Open-Access-Veröffentlichungen verfügbar gemacht werden, sowohl aggregiert über den gesamten Publikationszeitraum, als auch aufgeschlüsselt für die einzelnen Publikationsjahre [@COKI_date].

Die Version vom 18.08.2025 des COKI Open Access Dashboard umfasst Daten für 227 Länder und 56.187 Institutionen weltweit [@COKI_how]. Die bibliographischen Daten stammen aus mehreren offenen Datenquellen und berücksichtigen den Publikationszeitraum 2000--2024, zur Identifikation von Publikationen wird insbesondere Crossref genutzt [@COKI_how;@COKI_dashboard]. Die Auswahl der Publikationstypen ist im Vergleich zu den anderen Rankings weniger restriktiv, ausgeschlossen werden lediglich Publikationen mit den Typen Datensätze, Datenbanken, Komponenten, Berichtskomponenten, Peer Reviews, Fördermittel, Tagungsbände, Zeitschriftenausgaben, Berichtsreihen, Buchverzeichnisse, sowie alle Publikationen ohne angegebenen Publikationstyp [@COKI_how]. Institutionen müssen mindestens 50 Publikationen aufweisen, um in das COKI Open Access Dashboard aufgenommen zu werden, es sei denn, die Institutionen waren bereits im Datensatz aufgenommen, als dieser noch auf Microsoft Academic Graph basierte [@COKI_how].

Das COKI Open Access Dashboard legt den Fokus ausschließlich auf Open-Access-Aktivitäten der Institutionen und Länder. Während in der Listenansicht vorwiegend die absolute Anzahl sowie den prozentuale Anteil an Open-Access-Veröffentlichungen dargestellt wird, zeigen Detailansichten zu einzelnen Institutionen und Ländern differenziertere Aufschlüsselungen zu Publikationsaktivitäten und -orten, ergänzt durch Visualisierungen [@COKI_dashboard]. Um den Open-Access-Status von Publikationen zu bestimmen, nutzt das COKI Open Access Dashboard Unpaywall als Datenquelle [@COKI_how].


::: {.callout-note collapse="true"}
## Ranking Details

**Version:** 18.08.2025

**Website:** <https://open.coki.ac/>

**Ersteller:** [Curtin Open Knowledge Initiative](https://openknowledge.community/)

**Ranking Typ:** Länder- und Institutionen-Ranking

**Sektoren:** Förderorganisationen, Behörden, Hochschulen, Unternehmen, Gesundheitsorganisationen, Non-Profit Organisationen, Infrastrukturorganisationen, Archive, Sonstige

**Link zur Methodik:** <https://open.coki.ac/how/>

**Datenquellen:** Crossref (Publikationen), Unpaywall (Open Access Status)

**Dimension mit Open Access Indikatoren (Gewichtung):** --

**Open Access Indikatoren (Gewichtung):**

- Prozentualer Anteil an Open Access Veröffentlichungen
- Gesamtanzahl der Veröffentlichungen
- Gesamtanzahl an Open Access Veröffentlichungen
- Prozentualer Anteil an Artikeln, die über den Verlag Open Access zugänglich sind
- Prozentualer Anteil an Artikeln, die über andere Platformen Open Access zugänglich sind (Repositorien, Preprint-Server, etc.)
- Prozentualer Anteil an Artikeln, die sowohl über den Verlag, als auch über andere Platformen Open Access zugänglich sind
- Prozentualer Anteil an Closed Artikeln
- Prozentualer Anteil an Artikeln, die in einer Open Access Zeitschrift veröffentlicht wurden
- Prozentualer Anteil an Artikeln, die in einer Hybrid Zeitschrift
- Prozentualer Anteil an Artikeln, die über den Verlag ohne Wiederverwendungsrechte Open Access sind
- Prozentualer Anteil an Artikeln, die über institutionelle Repositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über Preprint Server Open Access sind
- Prozentualer Anteil an Artikeln, die über Fachrepositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über anderweitige, öffentliche Repositorien Open Access sind
- Prozentualer Anteil an Artikeln, die über anderweitige Webseiten oder Portale Open Access sind

**Publikations-/Update-Rythmus:** regelmäßig

**Anzahl an Institutionen:** 56.187 Institutionen

**Geographische Abdeckung:** global

**Darstellung:** Multi-Indikatoren Dashboard

**Datenverfügbarkeit:** Ergebnisdaten verfügbar über die Ranking Website unter CC-BY Lizenz: @COKI2025_07_21

:::


## Vergleichbarkeit der Ranking-Ergebnisse

Um zu beurteilen, wie vergleichbar die Ergebnisse der ausgewählten Rankings sind, konzentrieren sich die folgenden Abschnitte auf die Überprüfung folgender Aspekte: die von den Rankings verwendeten Datenquellen, die von den Rankings verwendeten Open-Access-Indikatoren und deren Gewichtung, die Anzahl der in allen Rankings vertretenen Institutionen, die geografische Verteilung der Institutionen über Länder und Weltregionen hinweg sowie die Möglichkeit der institutionellen Beteiligung am Datenerhebungsprozess der Ranking-Anbieter.

Es soll dabei herausgearbeitet werden, inwiefern sich die einzelnen Rankingsysteme unterscheiden und welche Auswirkungen dies mitunter für die Nutzung eines bestimmten Rankings bei Open-Access-Monitoringaktivitäten oder der strategischen Entscheidungsfindung haben kann. 

### Datenquellen


| Ranking |Web of Science | Scopus | OpenAlex | Crossref |
|:---------|:---------------:|:--------:|:----------:|:----------:|
| Leiden Ranking |  x | | | |
| Leiden Ranking Open Edition | | | x | |
| SCImago Institutions Ranking | | x | | |
| QS Sustainability Ranking | | x | | |
| COKI OA Dashboard | | | | x |

: Verwendete Datenquellen für die Erhebung bibliographischer Daten {#tbl-1}


#### Disziplinäre Unterschiede


::: {.column-page}

```{shinylive-r}
#| standalone: true
#| viewerHeight: 600

library(shiny)
library(bslib)
library(dplyr)
library(plotly)
library(viridis)

# Define UI ----
ui <- page_sidebar(
  sidebar = sidebar(open = "open",
      selectInput("field", "Field", choices = NULL),
      selectInput("period", "Period", choices = NULL, selected = "2019–2022"),
      selectInput("country", "Country", choices = NULL, multiple = TRUE), selected = "All",
      selectInput("indicator", "OA Indicator", choices = c(
        "P" = "oa_P", 
        "P(OA)" = "P_OA", 
        "P(gold OA)" = "P_gold_OA",
        "P(hybrid OA)" = "P_hybrid_OA", 
        "P(bronze OA)" = "P_bronze_OA", 
        "P(green OA)" = "P_green_OA", 
        "P(unknown OA)" = "P_OA_unknown", 
        "PP(OA)" = "PP_OA", 
        "PP(gold OA)" = "PP_gold_OA", 
        "PP(hybrid OA)" = "PP_hybrid_OA", 
        "PP(bronze OA)" = "PP_bronze_OA", 
        "PP(green OA)" = "PP_green_OA", 
        "PP(unknown OA)" = "PP_OA_unknown"
      ))
    ),
    mainPanel(
      plotlyOutput("scatter_plot", height = 500, width = 900)
    )
  )

server <- function(input, output, session) {
  
  # Load data once when app starts
  data_loaded <- reactiveVal(FALSE)
  leiden_shiny <- reactiveVal(NULL)
  
  # Initialize data only once
  observe({
    if (!data_loaded()) {
      data_url <- "https://oa-datenpraxis.de/rankings/data/leiden_shiny.csv"
      download.file(data_url, "leiden_shiny.csv", quiet = TRUE)
      data <- read.csv("leiden_shiny.csv")
      leiden_shiny(data)
      data_loaded(TRUE)

      updateSelectInput(session, "field", choices = unique(data$Field))
      updateSelectInput(session, "period", choices = unique(data$Period), selected = "2019–2022")
      updateSelectInput(session, "country", choices = c("All",sort(unique(data$Country))), selected = "All")
    }
  })
  
  # Filtered data reactive
  filtered_data <- reactive({
    req(leiden_shiny())
    req(input$field != "")
    req(input$period != "")
    
    # Get base data
    data <- leiden_shiny() %>%
      filter(Field == input$field) %>%
      filter(Period == input$period)
    
    ## If "All" is selected, return all data immediately
    if (length(input$country) == 1 && input$country == "All") {
      return(data)
    } else {
      # Filter by selected countries (excluding "All" if present)
      countries_to_filter <- input$country[input$country != "All"]
      if (length(countries_to_filter) > 0) {
        return(data %>% filter(Country %in% countries_to_filter))
      } else {
        return(data)  # Default to all data if no valid countries
      }
    }
  })
  
  x_var <- reactive({
    paste0(input$indicator, "_lr")
  })
  
  y_var <- reactive({
    paste0(input$indicator, "_lroe")
  })
  
  output$scatter_plot <- renderPlotly({
    req(filtered_data())
    req(nrow(filtered_data()) > 0, "No data to display")
    req(x_var() %in% colnames(filtered_data()), "X Variable not found in filtered data")
    req(y_var() %in% colnames(filtered_data()), "Y Variable not found in filtered data")
    
    # Get the range for both axes
    x_values <- filtered_data()[[x_var()]]
    y_values <- filtered_data()[[y_var()]]
    
    # Calculate max values for both axes
    max_x <- max(x_values, na.rm = TRUE)
    max_y <- max(y_values, na.rm = TRUE)
    
    # Add 5% padding to both axes
    max_x_padded <- max_x * 1.05
    max_y_padded <- max_y * 1.05
    
    max_val <- max(max_x_padded, max_y_padded)
    
    # Create indicator label mapping
    indicator_labels <- c(
      "oa_P" = "P",
      "P_OA" = "P(OA)",
      "P_gold_OA" = "P(gold OA)",
      "P_hybrid_OA" = "P(hybrid OA)",
      "P_bronze_OA" = "P(bronze OA)",
      "P_green_OA" = "P(green OA)",
      "P_OA_unknown" = "P(unknown OA)",
      "PP_OA" = "PP(OA)",
      "PP_gold_OA" = "PP(gold OA)",
      "PP_hybrid_OA" = "PP(hybrid OA)",
      "PP_bronze_OA" = "PP(bronze OA)",
      "PP_green_OA" = "PP(green OA)",
      "PP_OA_unknown" = "PP(unknown OA)"
    )
    
    # Get the human-readable label for the selected indicator
    selected_indicator_label <- indicator_labels[input$indicator]
    
    shapes <- list(
      list(
          type = "line",
          x0 = 0,
          y0 = 0,
          x1 = max_val,
          y1 = max_val,
          line = list(color = "rgba(0, 0, 0, 0.3)", width = 1),
          layer = "below"
        )
    )
    
    # Create plotly plot with hover information
    p <- plot_ly(filtered_data(), 
                 x = ~filtered_data()[[x_var()]], 
                 y = ~filtered_data()[[y_var()]],
                 text = ~paste0("<b>", University, "</b>", "\n", "Country: ", Country, "\n", "Field: ", Field, "\n", "Period: ", Period, "\n", "Indicator: ", selected_indicator_label),
                 color = ~Country,
                 colors = viridis(195),
                 hovertemplate = paste0(
                   "%{text}<br>",
                   "%{yaxis.title.text}: %{y:.2f}<br>",
                   "%{xaxis.title.text}: %{x:.2f}<br>",
                   "<extra></extra>"
                 ),
                 type = 'scatter',
                 mode = 'markers',
                 marker = list(size = 10))
    
    p <- p %>% layout(
      xaxis = list(title = "Leiden Ranking 2024 (Web of Science)"),
      yaxis = list(title = "Leiden Ranking Open Edition 2024 (OpenAlex)"),
      shapes = shapes,
      showlegend = FALSE
    )
    
    p
  })
}

# Create Shiny app ----
shinyApp(ui = ui, server = server)
```

:::



### Open-Access-Indikatoren



### Überschneidung von Institutionen

```{r}
#| label: loading-data
#| echo: false
#| warning: false
#| message: false

library(here)
library(tidyverse)
options(scipen = 999)

institutions_df_region <- read_csv(here("rankings/data","institutions_df_region.csv"))
institutions_df_education <- read_csv(here("rankings/data", "institutions_df_education.csv"))
```

Zur Bestimmung der institutionellen Überschneidungen zwischen den Rankings wurden zunächst die verfügbaren Ergebnisdaten der Rankings heruntergeladen[^1]. Neben den Namen und Standorten (Länderangaben, teilweise auch Angaben zu Regionen und Unterregion) aller Institutionen, enthalten die Daten der beiden Versionen des Leiden Rankings sowie des COKI OA Dashboards auch Identifier aus dem [Research Organization Registry (ROR)](https://ror.org/). Darüber hinaus umfassen die Daten des COKI OA Dashboard und des SCImago Institutions Ranking Sektorenzuordnungen für alle in den Rankings abgebildeten Institutionen. 

Um identische Institutionen zwischen den Rankings anhand ihrer ROR IDs eindeutig zuordnen zu können, mussten zunächst die Daten des SCImago Institutions Rankings und des QS Sustainability Rankings um ROR IDs ergänzt werden. Dazu wurde der gesamte ROR-Datensatz in der Version 1.69 [@ror_data] heruntergeladen. Anschließend wurde jeweils für beide Rankings ein maschineller Abgleich auf Basis der institutionellen Namensansetzung und der Länderangabe mit den ROR-Daten durchgeführt. Die QS Daten enthielten Einträge für insgesamt 1.743 Institutionen[^2], von denen 1.204 Institutionen mittels des maschinellen Abgleichs ROR IDs zugeordnet werden konnten. Für die restlichen 539 Institutionen wurde eine manuelle Zuordnung der ROR IDs vorgenommen, wobei allerdings für 7 Institutionen keine Zuordnung erfolgen konnte. Zur quantitativen Bestimmung der institutionellen Überschneidungen konnten demnach lediglich 1.736 Institutionen aus dem QS Sustainability Ranking berücksichtigt werden. 

Schwieriger gestaltete sich die Zuordnung von ROR IDs zu den 9.755 im SCImago Institutions Ranking inkludierten Institutionen[^3]. Über den maschinellen Abgleich auf Basis der institutionellen Namensansetzung und der Länderangabe konnten ROR IDs lediglich zu 4.714 Institutionen zugeordnet werden. In Teilen könnte dies darauf zurückzuführen sein, dass das SCImago Institutions Ranking auch Institutionen listet, die als multinational gekennzeichnet sind [@SIR_method]. Dies scheint insbesondere Unternehmen zu betreffen, die überregional agieren, beispielsweise [Google](https://www.scimagoir.com/institution.php?idp=89900). Zwar bietet ROR auch IDs für Unternehmen (siehe [Google](https://ror.org/00njsd438)), scheint dabei jedoch eindeutige Länderzuordnungen vorzunehmen. Die größere Schwierigkeit für den maschinellen Abgleich stellen jedoch vermutlich unterschiedliche Namensansetzungen zwischen dem SCImago Institutions Ranking und ROR dar[^4]. Um die Abdeckung mit ROR IDs für das SCImago Institutions Ranking zu erhöhen, wurde ein fuzzy matching Ansatz getestet, der auch alternative Namensformen und Akronyme aus dem ROR-Datensatz berücksichtigte. Da das Ergebnis jedoch nicht hinreichend genau war und einen hohen Datenbereinigungsaufwand erfordert hätte, wurde an dieser Stelle entschieden, lediglich die 4.714 Institutionen zu berücksichtigen, denen eindeutig ROR IDs zugeordnet werden konnten. Die Bestimmung der institutionellen Überschneidungen kann daher in Bezug auf das SCImago Institutions Ranking nur als ungefähre Annäherung betrachtet werden und stellt eine deutliche Limitation für die weitere Verwendbarkeit des SCImago-Datensatzes für diese Art von Analyse dar. 

@fig-2 stellt quantitativ die institutionellen Überschneidungen zwischen den Rankings dar. Die Abbildung bezieht dabei nur diejenigen Institutionen ein, denen ROR IDs zugeordnet werden konnten.

![Institutionelle Überschneidungen für alle Institutionen mit ROR IDs in fünf Rankingsystemen](rankings/plots/all_institution_upset.png){#fig-2 group="upset"}

Insgesamt konnten nach dem Abgleich der ROR IDs `r format(n_distinct(institutions_df_region$ror), big.mark=".", decimal.mark = ",")` einzigartige Instiutionen über alle Rankingsysteme hinweg identifiziert werden. Anhand von @fig-2 ist zu erkennen, dass das COKI OA Dashboard deutlich mehr Institutionen abbildet, als alle anderen Rankingsysteme, sowohl insgesamt (`r format(sum(institutions_df_region$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(institutions_df_region$coki) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %), als auch in Bezug auf die Anzahl an Institutionen, die in keinem anderen Rankingsystem abgebildet werden (`r format(sum(filter(institutions_df_region, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(filter(institutions_df_region, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %). Dies könnte insbesondere darauf zurückzuführen sein, dass das COKI OA Dashboard zum einen relativ langen Publikationszeitraum abbildet (2000--2024) und zum anderen auch Institutionen aus mehreren Sektoren, sowie Institutionen mit einem relativ geringen Publikationsvolumen von i.d.R. mindestens 50 Publikationen in das Ranking aufnimmt [@COKI_how;@COKI_data].

@fig-2 zeigt ferner, dass von den `r format(sum(institutions_df_region$sir), big.mark=".", decimal.mark = ",")` aus dem SCImago Institutions Ranking berücksichtigten Institutionen `r format(sum(filter(institutions_df_region, sir == 1 & lr == 0 & lroe == 0 & qs == 0 & coki == 0)$sir), big.mark=".", decimal.mark = ",")` Institutionen (`r format(round(sum(filter(institutions_df_region, sir == 1 & lr == 0 & lroe == 0 & qs == 0 & coki == 0)$sir) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %) in keinem anderen Ranking erfasst sind. Von den `r format(sum(institutions_df_region$qs), big.mark=".", decimal.mark = ",")` aus dem QS Sustainability Ranking berücksichtigten Institutionen sind `r format(sum(filter(institutions_df_region, sir == 0 & lr == 0 & lroe == 0 & qs == 1 & coki == 0)$qs), big.mark=".", decimal.mark = ",")` Institutionen (`r format(round(sum(filter(institutions_df_region, sir == 0 & lr == 0 & lroe == 0 & qs == 1 & coki == 0)$qs) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %) in keinem anderen Ranking erfasst. Dahingegen werden alle Instiutionen aus beiden Versionen des Leiden Rankings auch in mindestens einem weiteren Ranking abgebildet. Insgesamt sind lediglich `r format(sum(filter(institutions_df_region, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki), big.mark=".", decimal.mark = ",")` Institutionen (`r format(round(sum(filter(institutions_df_region, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %) in allen fünf Rankingsystemen erfasst. 

Während das QS Sustainability Ranking sowie beide Versionen des Leiden Ranking lediglich Hochschulen in ihre Rankings einbeziehen, sind im COKI OA Dashboard und dem SCImago Institutions Ranking auch Institutionen aus anderen Sektoren erfasst. Die Zuordnung der Institutionen zu einzelnen Sektoren bzw. die Kategorisierung der Institutionstypen im gesamten Datensatz erfolgte anhand der Klassifikation aus dem COKI OA Dashboard. Institutionen aus dem QS Sustainability Ranking, die nicht im COKI OA Dashboard repräsentiert sind wurden dem Bildungssektor (*Education*) zugeordnet. Institutionen aus dem SCImago Institutions Ranking, die nicht im COKI OA Dashboard repräsentiert sind, wurde der im SCImago Institutions Ranking hinterlegte Sektor zugeordnet. Um die institutionellen Überschneidungen bezogen auf den Bildungssektor zu untersuchen, wurde der gesamte Datensatz nach Institutionen mit der Sektorzuordnung *Education* gefiltert.

@fig-3 stellt im folgenden die institutionellen Überschneidungen der Bildungseinrichtungen zwischen den Rankings dar. Die Abbildung bezieht dabei ebenfalls nur diejenigen Institutionen ein, denen ROR IDs zugeordnet werden konnten.

![Institutionelle Überschneidungen im Bildungssektor für Institutionen mit ROR IDs in fünf Rankingsystemen](rankings/plots/education_institution_upset.png){#fig-3 group="upset"}

Insgesamt konnten `r format(n_distinct(institutions_df_education$ror), big.mark = ".")` einzigartige Institutionen aus dem gesamten Datensatz identifiziert werden, die dem Bildungssektor zugeordnet sind. @fig-3 zeigt, dass das COKI OA Dashboard auch in Bezug auf die Bildungsinstitutionen deutlich mehr Institutionen abbildet, als alle anderen Rankingsysteme, sowohl insgesamt (`r format(sum(institutions_df_education$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(institutions_df_education$coki) / n_distinct(institutions_df_education$ror) *100, 2), decimal.mark=",")` %), als auch in Bezug auf die Anzahl an Institutionen, die in keinem anderen Rankingsystem erfasst sind (`r format(sum(filter(institutions_df_education, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(filter(institutions_df_education, coki == 1 & lr == 0 & lroe == 0 & qs == 0 & sir == 0)$coki) / n_distinct(institutions_df_education$ror) *100, 2), decimal.mark=",")` %). Die Anzahl an Institutionen, die in allen fünf Rankingsystemen erfasst sind ist unverändert (`r format(sum(filter(institutions_df_education, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki), big.mark=".", decimal.mark = ",")` Institutionen, `r format(round(sum(filter(institutions_df_education, coki == 1 & lr == 1 & lroe == 1 & qs == 1 & sir == 1)$coki) / n_distinct(institutions_df_region$ror) *100, 2), decimal.mark=",")` %). 

@fig-3 zeigt jedoch auch, dass die Gesamtmenge an Institutionen (*Set size*) in beiden Versionen des Leiden Ranking und im QS Sustainability Ranking nach dem Filtern des Datensatzes auf Bildungseinrichtungen leicht minimiert ist. Dies ist auf die Kategorisierung der Institutionen innerhalb des COKI OA Dashboard Datensatzes zurückzuführen. Informationen zu den Institutionen im COKI Datensatz stammen aus dem Research Organization Registry (ROR) [@COKI_how]. Das ROR vergibt mindestens einen Institutionstyp aus einer kontrollierten Liste, wobei jedoch auch mehrere Typkategorien einer Institution zugeordnet werden können[^5] [@ror_types]. Im COKI Datensatz ist nur eine Typkategorie pro Instiution vorhanden, aus der Methodik geht auch nicht hervor nach welchen Kriterien die Typkategorie im Falle mehrer Kategorien ausgewählt wird. Dies könnte teilweise ursächlich dafür sein, dass weniger Institutionen aus dem QS Sustainability Ranking und den beiden Versionen der Leiden Rankings dem Bildungssektor zugeordnet wurden. Ferner könnten auch unterschiedliche Perspektiven zwischen ROR und den Rankings oder ggf. sogar fehlende Typkategorien in ROR ursächlich sein. 



[^1]: Informationen zur jeweiligen Datenverfügbarkeit inklusive eines Links zum Download der verwendeten Version der Daten -- sofern vorhanden -- sind für alle Rankings in dem jeweiligen Kasten mit den Ranking Details erfasst.
[^2]: @QS_date berichtet, dass 1.744 Instiutionen in der Version 2025 geranked wurden. Die Daten zeigen hier jedoch eine Abweichung um eine Einrichtung.
[^3]: Die bereitgestellten Daten zeigen eine Abweichung um eine Institution zu der von SCImago auf der Rankingwebsite berichteten Anzahl an gerankten Institutionen [@SIR]. Die bereitgestellte csv-Datei enthält zwar 9.756 Zeilen, jedoch handelt es sich bei der ersten Zeile lediglich um eine Kopfzeile. 
[^4]: Beispielsweise wird im SCImago Institutions Ranking das [Deutsche Krebsforschungszentrum](https://www.scimagoir.com/institution.php?idp=21592) in der Landessprache angesetzt, während in ROR die englische Namensvariante [German Cancer Research Center](https://ror.org/04cdgtt98) als Hauptansetzung verwendet wird.
[^5]: Siehe zum Beispiel [University of Nebraska Medical Center](https://ror.org/00thqtb16).

### Geographische Verteilung

::: {layout-ncol=2}

![](rankings/plots/lr_map.png){group="map"}

![](rankings/plots/lroe_map.png){group="map"}

:::

::: {layout-ncol=3}

![](rankings/plots/sir_univ_map.png){group="map"}

![](rankings/plots/qs_map.png){group="map"}

![](rankings/plots/coki_univ_map.png){group="map"}

:::


### Institutionelle Beteiligung bei der Datenerhebung

